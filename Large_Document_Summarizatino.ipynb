{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPEQ7OwhGUFaMQQhM4REqEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2dde1cb63fa4c23848150f18d774c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c8289c6ff83476db02a809fd083e4d1",
              "IPY_MODEL_0dacaa93e763448997130dafa8352bb7",
              "IPY_MODEL_9faf6c50f6074807b9812c6acfc8c176"
            ],
            "layout": "IPY_MODEL_e0b07f45ad4b42fb8156fefeac18e3b4"
          }
        },
        "9c8289c6ff83476db02a809fd083e4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa6e5962bb2d456f8a34ee5e3f92a36c",
            "placeholder": "​",
            "style": "IPY_MODEL_9d97231fde2446f4b57f3b0b639fe516",
            "value": "Map: 100%"
          }
        },
        "0dacaa93e763448997130dafa8352bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9584871d204740358bfc60d18a943ec1",
            "max": 84,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d03aa8515d35448b8482ed6425cd8e08",
            "value": 84
          }
        },
        "9faf6c50f6074807b9812c6acfc8c176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aa425df565c47a4921652775a7459db",
            "placeholder": "​",
            "style": "IPY_MODEL_9e8556cc0a944848ae6d9a3780af5737",
            "value": " 84/84 [00:06&lt;00:00, 12.88 examples/s]"
          }
        },
        "e0b07f45ad4b42fb8156fefeac18e3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6e5962bb2d456f8a34ee5e3f92a36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d97231fde2446f4b57f3b0b639fe516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9584871d204740358bfc60d18a943ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03aa8515d35448b8482ed6425cd8e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7aa425df565c47a4921652775a7459db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e8556cc0a944848ae6d9a3780af5737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe8413bac1494f35addaf6c62cc6d2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e72f37ac1ad4a018daed2ac32b9ca77",
              "IPY_MODEL_3a53b79757da43738ada5f8d35c1a07d",
              "IPY_MODEL_c1d1c03836f2401abee06528d670ece1"
            ],
            "layout": "IPY_MODEL_0af62618b18a4677b0b0a48dd1e43f11"
          }
        },
        "0e72f37ac1ad4a018daed2ac32b9ca77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a7f2d6987648f5b3fa3ab35e43e65a",
            "placeholder": "​",
            "style": "IPY_MODEL_bbea22768ce24173b508358a6096fb45",
            "value": "Map: 100%"
          }
        },
        "3a53b79757da43738ada5f8d35c1a07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0f98e51c0fa4b2dbe386794c269900b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1cd04469be444039bd5f6521df0edd0",
            "value": 10
          }
        },
        "c1d1c03836f2401abee06528d670ece1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_308e8d6388544af486d1f6998d2ec204",
            "placeholder": "​",
            "style": "IPY_MODEL_969cdbf6343942a6a5c2d7f2842f86b8",
            "value": " 10/10 [00:01&lt;00:00,  8.89 examples/s]"
          }
        },
        "0af62618b18a4677b0b0a48dd1e43f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a7f2d6987648f5b3fa3ab35e43e65a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbea22768ce24173b508358a6096fb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0f98e51c0fa4b2dbe386794c269900b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1cd04469be444039bd5f6521df0edd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "308e8d6388544af486d1f6998d2ec204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "969cdbf6343942a6a5c2d7f2842f86b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e79699200e74792aab0186e0a206e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_607210f5311b444c833b4efac4c57920",
              "IPY_MODEL_1f19a668dafb4e7f8d1f337de2d5d920",
              "IPY_MODEL_5c07f80af7ec4cab8563cb8f99028ca7"
            ],
            "layout": "IPY_MODEL_af5c9e2a0a1a4f15848152d5391fad92"
          }
        },
        "607210f5311b444c833b4efac4c57920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ccc4eed4114f3e9acc0b07476f1a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_b3b7cd3d8e0e4af3ba869acb964170af",
            "value": "Downloading builder script: 100%"
          }
        },
        "1f19a668dafb4e7f8d1f337de2d5d920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e370b655920947a189b5656687d79279",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4d023e6d09c4f4ea71345a20b8332de",
            "value": 6270
          }
        },
        "5c07f80af7ec4cab8563cb8f99028ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_410ea1f6654541aea65f8c6ca07e41a9",
            "placeholder": "​",
            "style": "IPY_MODEL_b4fb4125d3ac4f09bd78cff6d9d32f8d",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 77.6kB/s]"
          }
        },
        "af5c9e2a0a1a4f15848152d5391fad92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ccc4eed4114f3e9acc0b07476f1a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b7cd3d8e0e4af3ba869acb964170af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e370b655920947a189b5656687d79279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d023e6d09c4f4ea71345a20b8332de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "410ea1f6654541aea65f8c6ca07e41a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4fb4125d3ac4f09bd78cff6d9d32f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harish1208/CE156-Introduction-To-Python/blob/main/Large_Document_Summarizatino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXFvNxPZZB_o",
        "outputId": "e6c9f14c-6e67-45db-dfdf-4deefb68b9a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization & Stopwords**"
      ],
      "metadata": {
        "id": "P9ifmeROQ7nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/train/meeting_en_train_001/transcript_MAN_annot13.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHt-nzoyZGel",
        "outputId": "79624e13-6cbc-4e2a-d68a-9c3824fcc530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/train/meeting_en_train_001/transcript_MAN_annot13.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNhPxG6VaO3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/train/meeting_en_train_001/transcript_MAN_annot13.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    transcript = file.read()"
      ],
      "metadata": {
        "id": "2_hfaxwIZy8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Load the transcript from the specified file path\n",
        "file_path = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/train/meeting_en_train_001/transcript_MAN_annot13.txt'\n",
        "\n",
        "# Read the transcript from the file\n",
        "with open(file_path, 'r') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Function to identify unique speakers and their counts\n",
        "def find_speakers(text):\n",
        "    # Regex pattern to match speaker labels (e.g., (PERSON13), (PERSON6))\n",
        "    speaker_pattern = r'\\(PERSON\\d+\\)'  # Matches (PERSON1), (PERSON2), ..., (PERSONN)\n",
        "    speakers = re.findall(speaker_pattern, text)\n",
        "\n",
        "    # Count the occurrences of each speaker\n",
        "    speaker_counts = Counter(speakers)\n",
        "    unique_speakers = list(speaker_counts.keys())\n",
        "\n",
        "    return unique_speakers, speaker_counts\n",
        "\n",
        "# Apply the function to the transcript\n",
        "unique_speakers, speaker_counts = find_speakers(transcript)\n",
        "\n",
        "# Print results\n",
        "print(f\"Unique speakers found: {len(unique_speakers)}\")\n",
        "print(f\"Speaker counts: {speaker_counts}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be8Ti17Dh3cf",
        "outputId": "d4bebac9-67bf-485c-c5b7-96d8786c088a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique speakers found: 6\n",
            "Speaker counts: Counter({'(PERSON13)': 105, '(PERSON6)': 40, '(PERSON18)': 28, '(PERSON19)': 23, '(PERSON10)': 18, '(PERSON3)': 8})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"NLTK resources downloaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeGmuovtzj_g",
        "outputId": "d0a75638-d2ae-43c6-fd64-906d22fb3c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resources downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script 2: Load Text\n",
        "def load_transcript(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        transcript = file.read()\n",
        "    return transcript\n",
        "\n",
        "# # Example usage\n",
        "# file_path = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/train/meeting_en_train_001/transcript_MAN_annot13.txt'\n",
        "\n",
        "transcript = load_transcript(file_path)\n",
        "print(\"Transcript loaded successfully.\")\n",
        "\n",
        "word_count = len(transcript.split())\n",
        "\n",
        "# Print the word count\n",
        "print(f\"Total word count in the transcript: {word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAGYqdSc3kDu",
        "outputId": "0bb3b0eb-e913-4595-9758-87f78afc6fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript loaded successfully.\n",
            "Total word count in the transcript: 5378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure NLTK resources are downloaded (uncomment if running for the first time)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    # Tokenize the text into sentences\n",
        "    tokenize_transcript_sentences = sent_tokenize(text)\n",
        "    return tokenize_transcript_sentences\n",
        "\n",
        "def count_words(sentences):\n",
        "    # Count the total number of words in the tokenized sentences\n",
        "    total_word_count = sum(len(sentence.split()) for sentence in sentences)\n",
        "    return total_word_count\n",
        "\n",
        "# Assuming 'transcript' is the input text\n",
        "\n",
        "# Step 1: Tokenize the transcript into sentences\n",
        "sentences = tokenize_sentences(transcript)\n",
        "\n",
        "# Print the tokenized sentences\n",
        "\n",
        "# print(\"Tokenized Sentences:\")\n",
        "# for sentence in sentences:\n",
        "#     print(sentence)\n",
        "\n",
        "# Step 2: Count the total number of words after sentence tokenization\n",
        "total_word_count = count_words(sentences)\n",
        "\n",
        "# Print the total word count\n",
        "print(f\"Total word count after sentence tokenization: {total_word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIjP1PgEeUlZ",
        "outputId": "4b6c09ec-e0e4-4d55-e908-4cf813736445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count after sentence tokenization: 5378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure NLTK resources are downloaded (uncomment if running for the first time)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def tokenize_words(text):\n",
        "    # Tokenize the text into words\n",
        "    tokenized_transcript_words = word_tokenize(text)\n",
        "    return tokenized_transcript_words\n",
        "\n",
        "def count_words(tokenized_words):\n",
        "    # Count the total number of words in the tokenized words\n",
        "    total_word_count = len(tokenized_words)\n",
        "    return total_word_count\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Tokenize the transcript into words\n",
        "tokenized_words = tokenize_words(transcript)\n",
        "\n",
        "# Print the tokenized words\n",
        "\n",
        "\n",
        "# print(\"Tokenized Words:\")\n",
        "# for word in tokenized_words:\n",
        "#     print(word)\n",
        "\n",
        "# Step 2: Count the total number of words after word tokenization\n",
        "total_word_count = count_words(tokenized_words)\n",
        "\n",
        "# Print the total word count\n",
        "print(f\"Total word count after word tokenization: {total_word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1qWJ7DMDWzIM",
        "outputId": "65c3c927-0d42-4711-fc77-bc7e12aa2968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count after word tokenization: 7456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK resources are downloaded (uncomment if running for the first time)\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(tokenized_words):\n",
        "    # Load English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Filter out stopwords from the tokenized words\n",
        "    filtered_words = [word for word in tokenized_words if word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "def count_words(filtered_words):\n",
        "    # Count the total number of words after stopword removal\n",
        "    total_word_count = len(filtered_words)\n",
        "    return total_word_count\n",
        "\n",
        "# Assuming 'tokenized_words' is coming from the previous word tokenization script\n",
        "\n",
        "# Step 1: Remove stopwords from the tokenized words\n",
        "filtered_words = remove_stopwords(tokenized_words)\n",
        "\n",
        "# Print the filtered words (after stopword removal)\n",
        "\n",
        "\n",
        "print(\"Filtered Words (Stopwords Removed):\")\n",
        "for word in filtered_words:\n",
        "    print(word)\n",
        "\n",
        "# Step 2: Count the total number of words after stopword removal\n",
        "total_word_count = count_words(filtered_words)\n",
        "\n",
        "# Print the total word count\n",
        "print(f\"Total word count after stopword removal: {total_word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h1-K1DaO3-yQ",
        "outputId": "f3ef681c-d6ee-4928-dbbd-57db4b8ff9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words (Stopwords Removed):\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Hi\n",
            ".\n",
            "Hello\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ".\n",
            "Hello\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ".\n",
            "Thanks\n",
            ",\n",
            "uhm\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Hi\n",
            "everyone\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Hi\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "great\n",
            ".\n",
            "Thanks\n",
            "joining\n",
            ",\n",
            "uh\n",
            ",\n",
            "yeah\n",
            "okay\n",
            ".\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Uh\n",
            ",\n",
            "see\n",
            "people\n",
            "written\n",
            "ehm\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Hi\n",
            "[\n",
            "PERSON13\n",
            "]\n",
            ",\n",
            "hear\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yep\n",
            ",\n",
            "'s\n",
            "great\n",
            ".\n",
            "Uh\n",
            ",\n",
            "also\n",
            "evaluating-\n",
            ".\n",
            "Yes\n",
            ",\n",
            "'s\n",
            "'s\n",
            "record\n",
            ".\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "mind\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "well\n",
            ",\n",
            "uh\n",
            ",\n",
            "preparations\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON13\n",
            "]\n",
            ",\n",
            "uh\n",
            "busy\n",
            ",\n",
            "uh\n",
            ",\n",
            "IW\n",
            "SLT\n",
            ",\n",
            "uh\n",
            ",\n",
            "write-up\n",
            ".\n",
            "Uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "wra\n",
            "last\n",
            "part\n",
            ".\n",
            "busy\n",
            "interviewing\n",
            "people\n",
            "people\n",
            "uh\n",
            "replace\n",
            "em\n",
            "moving\n",
            "forward\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "say\n",
            ".\n",
            "number\n",
            "colleagues\n",
            "projects\n",
            "supervising\n",
            ",\n",
            "uh\n",
            ",\n",
            "going\n",
            "studies\n",
            "abroad\n",
            "things\n",
            ".\n",
            "Uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "think\n",
            "focus\n",
            "demo\n",
            "Project\n",
            "Officer\n",
            ".\n",
            "need\n",
            "focus\n",
            "ladder\n",
            "climbing\n",
            ",\n",
            "uh\n",
            ",\n",
            "building\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "test\n",
            "set\n",
            "plus\n",
            ",\n",
            "uh\n",
            ",\n",
            "regularly\n",
            ",\n",
            "uh\n",
            ",\n",
            "testing\n",
            ".\n",
            "Ehm\n",
            ",\n",
            ",\n",
            "ehm\n",
            "else\n",
            ",\n",
            "uh\n",
            ",\n",
            "deliverables\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "deliverables\n",
            ".\n",
            ",\n",
            "'ll\n",
            "try\n",
            "provide\n",
            "links-\n",
            ".\n",
            ",\n",
            "already\n",
            "working\n",
            "deliverables\n",
            ",\n",
            "please\n",
            "mention\n",
            ".\n",
            "yeah\n",
            ".\n",
            "Let\n",
            "'s\n",
            "let\n",
            "'s\n",
            "go\n",
            "quickly\n",
            "done\n",
            ".\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "first\n",
            "list\n",
            ".\n",
            "Ehm\n",
            ",\n",
            "ehm\n",
            ",\n",
            "please\n",
            "briefly\n",
            "update\n",
            "working\n",
            ".\n",
            "plan\n",
            "next\n",
            "week\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "<\n",
            "other_noise/\n",
            ">\n",
            ",\n",
            "luckily\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "luckily\n",
            "week\n",
            "like\n",
            "quite\n",
            "less\n",
            "tasks\n",
            ".\n",
            "first\n",
            "calculated\n",
            "word\n",
            "error\n",
            "rate\n",
            "Czech\n",
            "transcripts\n",
            "using\n",
            "three\n",
            "versions\n",
            ",\n",
            "uh\n",
            ",\n",
            "Czech\n",
            "ASR\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            "created\n",
            ".\n",
            "yesterday\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            "told\n",
            ",\n",
            "uh\n",
            ",\n",
            "golden\n",
            "transcript\n",
            "corresponding\n",
            "video\n",
            "huge\n",
            "huge\n",
            "mismatch\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "said\n",
            "update\n",
            ".\n",
            "conducted\n",
            "testing\n",
            "sessions\n",
            "[\n",
            "PERSON11\n",
            "]\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ".\n",
            "quite\n",
            "successful\n",
            "segmenters\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "ORGANIZATION1\n",
            "]\n",
            "still\n",
            "[\n",
            "PERSON12\n",
            "]\n",
            "today\n",
            "working\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "lastly\n",
            "yeah\n",
            "think\n",
            "did-\n",
            ".\n",
            "Uh\n",
            ",\n",
            "input\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "deliverable\n",
            "punctuator\n",
            "caser\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "n't\n",
            "have-\n",
            ".\n",
            "think\n",
            "apart\n",
            "testing\n",
            "sessions\n",
            "week\n",
            "waiting\n",
            "new\n",
            "tasks\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            ".\n",
            "word\n",
            "error\n",
            "rate\n",
            ",\n",
            "also\n",
            "English\n",
            ",\n",
            "uh\n",
            ",\n",
            "transcripts\n",
            "?\n",
            "Ehm\n",
            ",\n",
            "also\n",
            "[\n",
            "PERSON9\n",
            "]\n",
            "German\n",
            "one\n",
            ",\n",
            "right\n",
            "?\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            ",\n",
            "make\n",
            "do-\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "updated\n",
            "German\n",
            "transcripts\n",
            "corresponding\n",
            "path\n",
            "like\n",
            "golden\n",
            "transcripts\n",
            "English\n",
            "videos\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ",\n",
            "'s\n",
            "part\n",
            ".\n",
            "consecutively\n",
            "translated\n",
            "videos\n",
            ".\n",
            "always\n",
            "English\n",
            "speaker\n",
            "Czech\n",
            "speaker\n",
            "repeats\n",
            "content\n",
            ".\n",
            "[\n",
            "PERSON7\n",
            "]\n",
            "split\n",
            "video\n",
            "English\n",
            "part\n",
            "reliable\n",
            ",\n",
            "uh\n",
            ",\n",
            "Czech\n",
            "part\n",
            "done\n",
            "simply\n",
            "using\n",
            "ends\n",
            ".\n",
            "Czech\n",
            "video\n",
            "cut\n",
            "using\n",
            "English\n",
            "time\n",
            "stamps\n",
            ".\n",
            "Like\n",
            "end\n",
            "English\n",
            "beginning\n",
            "next\n",
            "English\n",
            "segment\n",
            ".\n",
            "Uh\n",
            ",\n",
            "'s\n",
            "like\n",
            "like\n",
            "interleave\n",
            "way\n",
            "round\n",
            ".\n",
            "'s\n",
            "'m\n",
            "surprised\n",
            "Czech\n",
            "video\n",
            ",\n",
            "uh\n",
            ",\n",
            "imprecise\n",
            "timing\n",
            ".\n",
            "still\n",
            ",\n",
            "expecting\n",
            "bad\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "something\n",
            ",\n",
            "yeah\n",
            ".\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            ",\n",
            "maybe\n",
            "tell\n",
            "us\n",
            "details\n",
            "?\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Well\n",
            ",\n",
            "like\n",
            "listened\n",
            "audio\n",
            "followed\n",
            "talk\n",
            "transcript\n",
            "<\n",
            "other_noise/\n",
            ">\n",
            "completely\n",
            ".\n",
            "think\n",
            "is-\n",
            ".\n",
            "must\n",
            "miss-match\n",
            "because-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "transcription\n",
            "completely\n",
            "different\n",
            "audio\n",
            "'s\n",
            "subdirectory\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Oh\n",
            ",\n",
            "someone\n",
            "must\n",
            "like\n",
            "messed\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "may\n",
            "maybe\n",
            "'s\n",
            "like\n",
            "uh\n",
            ".\n",
            "Maybe\n",
            "files\n",
            "switched\n",
            "subdirectories\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "n't\n",
            "checked\n",
            "but-\n",
            ".\n",
            "Uh\n",
            ",\n",
            "yeah\n",
            ",\n",
            "serious\n",
            "mismatch\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            "coul\n",
            "could\n",
            "check\n",
            "?\n",
            "hard\n",
            "Like\n",
            "try\n",
            "listening\n",
            "files\n",
            "within\n",
            "demo\n",
            "[\n",
            "PERSON15\n",
            "]\n",
            ",\n",
            "uh\n",
            ",\n",
            "try\n",
            "locate\n",
            "correct\n",
            "file\n",
            ",\n",
            "appropriate\n",
            "files\n",
            ".\n",
            ",\n",
            "transcripts\n",
            "ready\n",
            ".\n",
            "able\n",
            ",\n",
            "uh\n",
            ",\n",
            "evaluate\n",
            ".\n",
            "also\n",
            "English\n",
            "ones\n",
            "translations\n",
            ".\n",
            "English\n",
            "ones\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ",\n",
            "uh\n",
            ",\n",
            "would\n",
            "like\n",
            "evaluate\n",
            "word\n",
            "error\n",
            "rate\n",
            "ASR\n",
            ".\n",
            "also\n",
            "machine\n",
            "translation\n",
            "quality\n",
            "SLT\n",
            "even\n",
            ".\n",
            "Uh\n",
            ",\n",
            "translation\n",
            "quality\n",
            "German\n",
            "Czech\n",
            ".\n",
            "available\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "files\n",
            "ready\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            ",\n",
            "like\n",
            "German\n",
            "audio\n",
            "English\n",
            "[\n",
            "PROJECT1\n",
            "]\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "English\n",
            ",\n",
            "uh\n",
            ",\n",
            "input\n",
            "English\n",
            "sound\n",
            ".\n",
            "golden\n",
            "English\n",
            "transcript\n",
            ",\n",
            "check\n",
            "ASR\n",
            ".\n",
            "also\n",
            "translation\n",
            "Czech\n",
            "German\n",
            ".\n",
            "also\n",
            "evaluate\n",
            "directly\n",
            "translation\n",
            "quality\n",
            ",\n",
            "uh\n",
            ",\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Okay\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            ",\n",
            "important\n",
            ",\n",
            "uh\n",
            ",\n",
            "task\n",
            ",\n",
            "uh\n",
            ",\n",
            ",\n",
            "uh\n",
            "wr\n",
            "also\n",
            "German\n",
            "English\n",
            "audios\n",
            ".\n",
            "another\n",
            ",\n",
            "uh\n",
            ",\n",
            "bleu\n",
            "SLTF\n",
            ",\n",
            "uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "German\n",
            "Czech\n",
            "translations\n",
            ",\n",
            "uh\n",
            ",\n",
            "English\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "ASR\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Audio\n",
            "input\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Uh\n",
            ",\n",
            "okay\n",
            ".\n",
            ",\n",
            "testing\n",
            "sessions\n",
            "uhm\n",
            "uh\n",
            ",\n",
            "successful\n",
            "like\n",
            "reliable\n",
            ",\n",
            "uh\n",
            ",\n",
            "testing\n",
            "sessions\n",
            "?\n",
            "[\n",
            "PERSON11\n",
            "]\n",
            "say\n",
            "[\n",
            "PERSON12\n",
            "]\n",
            "say\n",
            "anything\n",
            "new\n",
            "date\n",
            "?\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            ",\n",
            "like\n",
            "yesterday\n",
            "holiday\n",
            "Germany\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            ",\n",
            "like\n",
            "n't\n",
            "work\n",
            ".\n",
            "today\n",
            "[\n",
            "PERSON12\n",
            "]\n",
            "working\n",
            "morning\n",
            "n't\n",
            "updates\n",
            "[\n",
            "PERSON12\n",
            "]\n",
            "status\n",
            "of-\n",
            ".\n",
            ".\n",
            "yeah\n",
            ",\n",
            "today\n",
            "[\n",
            "PERSON12\n",
            "]\n",
            "fixings\n",
            "segmenter\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "please\n",
            "also\n",
            "check\n",
            "timing\n",
            ".\n",
            "Like\n",
            ",\n",
            "would\n",
            "available\n",
            ".\n",
            "Uh\n",
            ",\n",
            "[\n",
            "ORGANIZATION1\n",
            "]\n",
            "plus\n",
            "[\n",
            "ORGANIZATION6\n",
            "]\n",
            "expect\n",
            ",\n",
            "uh\n",
            ",\n",
            "uhm\n",
            ",\n",
            "uh\n",
            ",\n",
            "reliable\n",
            "run\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "Including\n",
            "segmenter\n",
            ".\n",
            "Yeah\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "yeah\n",
            "the-\n",
            ".\n",
            "provided\n",
            "input\n",
            "delive\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "deliverable\n",
            ",\n",
            "right\n",
            "?\n",
            "need\n",
            "review\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "'s\n",
            "good\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "another\n",
            "idea\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Uh\n",
            ",\n",
            "'s\n",
            ",\n",
            "uh\n",
            ",\n",
            "um\n",
            "question\n",
            "like\n",
            "backing\n",
            "systems\n",
            "net\n",
            "data\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "systems\n",
            ".\n",
            "done\n",
            "anything\n",
            "?\n",
            "full\n",
            "back-up\n",
            "systems\n",
            "?\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "including\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yes\n",
            ",\n",
            "already\n",
            "everything\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "okay\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            ".\n",
            "also\n",
            "today\n",
            "systems\n",
            ".\n",
            "created\n",
            ".\n",
            "also\n",
            "last\n",
            "week\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "okay\n",
            ".\n",
            "'s\n",
            "uh\n",
            "'s\n",
            "good\n",
            ".\n",
            "Uh\n",
            ",\n",
            "another\n",
            "thing\n",
            "finding\n",
            "another\n",
            "partner\n",
            ",\n",
            "uh\n",
            ",\n",
            "would\n",
            "ehm\n",
            "ehm\n",
            "run\n",
            "replicates\n",
            "replicas\n",
            "systems\n",
            ".\n",
            "imagine\n",
            "'s\n",
            "us\n",
            "without\n",
            "connectivity\n",
            ".\n",
            "need\n",
            "someone\n",
            "replace\n",
            "us\n",
            ",\n",
            "uh\n",
            ",\n",
            "-\n",
            ".\n",
            "Like\n",
            "able\n",
            "run\n",
            "systems\n",
            "[\n",
            "ORGANIZATION1\n",
            "]\n",
            ",\n",
            "someone-\n",
            ".\n",
            "need\n",
            "pick\n",
            "someone\n",
            ",\n",
            "uh\n",
            ",\n",
            "happy\n",
            "run\n",
            "systems\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON13\n",
            "]\n",
            "suggest\n",
            "[\n",
            "ORGANIZATION2\n",
            "]\n",
            "run\n",
            "[\n",
            "PROJECT1\n",
            "]\n",
            "models\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "please\n",
            "check\n",
            "[\n",
            "PERSON8\n",
            "]\n",
            "later\n",
            ",\n",
            "ehm-\n",
            ".\n",
            "[\n",
            "PERSON13\n",
            "]\n",
            "suggests\n",
            "[\n",
            "ORGANIZATION2\n",
            "]\n",
            "run\n",
            "uh\n",
            "uh\n",
            "[\n",
            "PROJECT1\n",
            "]\n",
            "models\n",
            ",\n",
            "[\n",
            "ORGANIZATION1\n",
            "]\n",
            "run\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "models\n",
            ",\n",
            "probably\n",
            ".\n",
            "uh\n",
            "uh\n",
            "well\n",
            "uh\n",
            "ehm\n",
            "someone\n",
            "run\n",
            "segmenters\n",
            ",\n",
            "yeah\n",
            ".\n",
            ",\n",
            "need\n",
            "pick\n",
            "someone\n",
            ".\n",
            ",\n",
            "could-\n",
            ".\n",
            "touch\n",
            ",\n",
            "uh\n",
            ",\n",
            "please\n",
            "talk\n",
            ",\n",
            "uh\n",
            ",\n",
            "let\n",
            "know\n",
            ".\n",
            "Thursday\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "call\n",
            "call\n",
            "would\n",
            "actually\n",
            "like\n",
            "know\n",
            "substitute\n",
            "site\n",
            "systems\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            "od\n",
            "check\n",
            "others\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            "tell\n",
            "uh\n",
            "tomm\n",
            "Thursday\n",
            "call\n",
            "know\n",
            "substitute\n",
            "systems\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Okay\n",
            ",\n",
            "thank\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "thank\n",
            ".\n",
            "lot\n",
            "work\n",
            "evaluation\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "would\n",
            "like\n",
            "really\n",
            "soon\n",
            "see\n",
            "numbers\n",
            "trust\n",
            ".\n",
            "obviously\n",
            "problem\n",
            "Czech\n",
            ".\n",
            "hopefully\n",
            "problems\n",
            "thing\n",
            ",\n",
            "uh\n",
            ",\n",
            "things\n",
            ".\n",
            "this-\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "believe\n",
            "Czech\n",
            "transcripts\n",
            "[\n",
            "PERSON7\n",
            "]\n",
            "probably\n",
            "might\n",
            ",\n",
            "uhm\n",
            ".\n",
            "need\n",
            "ask\n",
            "audio\n",
            "used\n",
            "transcribe\n",
            "videos\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "mean\n",
            ",\n",
            "must\n",
            "received\n",
            "audio\n",
            ",\n",
            "written\n",
            "transcripts\n",
            "manually\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            ",\n",
            "maybe\n",
            "'s\n",
            "real\n",
            "audio\n",
            "repla\n",
            "placed\n",
            "directory\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ",\n",
            ".\n",
            "someone\n",
            "understand\n",
            "Czech\n",
            "obviously\n",
            ",\n",
            "uh\n",
            ",\n",
            "find\n",
            ".\n",
            "audio\n",
            "input\n",
            ",\n",
            "uh\n",
            ",\n",
            "transcript\n",
            "corresponds\n",
            ",\n",
            "audio\n",
            "released\n",
            "IWSLT\n",
            "test\n",
            "set\n",
            ".\n",
            "anyone\n",
            ",\n",
            "uh\n",
            ",\n",
            "easily\n",
            "check\n",
            "IWSLT\n",
            "test\n",
            "set\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Okay\n",
            "thanks\n",
            ",\n",
            "thanks\n",
            ".\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ".\n",
            "working\n",
            "hard\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "like\n",
            "hard\n",
            "evaluation\n",
            "[\n",
            "PROJECT1\n",
            "]\n",
            "ASR\n",
            "submissions\n",
            "IWSLT\n",
            "last\n",
            "week\n",
            ".\n",
            "Uh\n",
            ",\n",
            "days\n",
            ",\n",
            "uh\n",
            ",\n",
            "working\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            "?\n",
            "still\n",
            "hear\n",
            "us\n",
            ".\n",
            "Uh\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Uh\n",
            "yes\n",
            ",\n",
            "hear\n",
            ".\n",
            "Hello\n",
            ".\n",
            "Uh\n",
            ".\n",
            "[\n",
            "PERSON13\n",
            "]\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "hear\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "hear\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "week\n",
            "working\n",
            "IWSLT\n",
            "submissions\n",
            "files\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "problem\n",
            "calculating\n",
            "bleu\n",
            "score\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "'ve\n",
            "sent\n",
            "email\n",
            ".\n",
            "think\n",
            "need\n",
            "good\n",
            "tokeniser\n",
            ".\n",
            "two\n",
            "ways\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "ready\n",
            "use\n",
            "tokeniser\n",
            "us\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "another\n",
            "problem\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "calculating\n",
            "bleu\n",
            "score\n",
            ".\n",
            "used\n",
            "NLTK\n",
            "model\n",
            "calculating\n",
            "scores\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "discussed\n",
            ".\n",
            "discussed\n",
            "NLTK\n",
            "versus\n",
            "SacreBLEU\n",
            "[\n",
            "PERSON2\n",
            "]\n",
            ".\n",
            "trust\n",
            "NLTK\n",
            "bleu\n",
            "score\n",
            ".\n",
            "probably\n",
            "tokenising\n",
            "also\n",
            "accidentally\n",
            "is-\n",
            ".\n",
            "drops\n",
            "0\n",
            ".\n",
            "is-\n",
            ".\n",
            ",\n",
            "inflated\n",
            "small\n",
            ".\n",
            "n't\n",
            "trust\n",
            "NLTK\n",
            "bleu\n",
            "scores\n",
            ".\n",
            "trust\n",
            "Sacre-BLEU\n",
            ".\n",
            "think\n",
            "'re\n",
            "like-\n",
            ".\n",
            "Please\n",
            "write\n",
            ".\n",
            "Oh\n",
            "have-\n",
            ".\n",
            "sent\n",
            "email\n",
            "?\n",
            "double\n",
            "check\n",
            ".\n",
            "Uh\n",
            ",\n",
            "Well\n",
            ",\n",
            "SLTF\n",
            "thank\n",
            "organisation\n",
            ".\n",
            "See\n",
            "later\n",
            ".\n",
            "SLT\n",
            "system\n",
            "mapping\n",
            ".\n",
            "n't\n",
            "see\n",
            "email\n",
            "there-\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            ".\n",
            "Hmm\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "want\n",
            ",\n",
            "resend\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Space\n",
            "tokeniser\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "Yes\n",
            ",\n",
            "es\n",
            "essentially\n",
            "answer\n",
            "question\n",
            "email\n",
            ".\n",
            "switch\n",
            "IWSLT\n",
            ".\n",
            "switch\n",
            "SacreBLEU\n",
            "SacreBLEU\n",
            "tokenisation\n",
            "scoring\n",
            ".\n",
            "no-\n",
            ".\n",
            "Let\n",
            "'s\n",
            "let\n",
            "'s\n",
            "simply\n",
            "forget\n",
            "NLTK\n",
            "bleu\n",
            "score\n",
            ".\n",
            "reliable\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yes\n",
            ",\n",
            "but-\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yes\n",
            ",\n",
            "combine\n",
            "tokeniser\n",
            "NLTK\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Uf\n",
            ".\n",
            "Let\n",
            "'s\n",
            ".\n",
            "Let\n",
            "'s\n",
            "forget\n",
            ".\n",
            "Let\n",
            "'s\n",
            "let\n",
            "'s\n",
            "use\n",
            "SacreBLEU\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "one\n",
            "comment\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "sh\n",
            "use\n",
            "tokeniser\n",
            "enverse\n",
            "segmenter\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ",\n",
            "'s\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "'s\n",
            "much\n",
            "better\n",
            ".\n",
            "rely\n",
            "dots\n",
            "commas\n",
            "question\n",
            "marks\n",
            ".\n",
            "check\n",
            "script\n",
            "tokeniser\n",
            ",\n",
            "enverse\n",
            "segmenter\n",
            "de-tokeniser\n",
            ".\n",
            "path\n",
            "document\n",
            ".\n",
            "And-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "'s\n",
            "'s\n",
            "using\n",
            "Moses\n",
            "seg\n",
            "tokeniser\n",
            "detokeniser\n",
            ".\n",
            "needs\n",
            "language\n",
            "tag\n",
            "first\n",
            "argument\n",
            "reference\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ",\n",
            "fo-\n",
            "?\n",
            "understand\n",
            "?\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "problem\n",
            "enverse\n",
            "segmenter\n",
            "reliable\n",
            "run\n",
            "non-tokenised\n",
            "text\n",
            ".\n",
            "much\n",
            "better\n",
            "tokenise\n",
            "enverse\n",
            "segmenter\n",
            "maybe\n",
            "undo\n",
            "tokenisation\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            "'s\n",
            "script\n",
            ".\n",
            "would\n",
            "like\n",
            "incorporate\n",
            "logic\n",
            "SLTF\n",
            ".\n",
            "SLTF\n",
            "run\n",
            "tokenisation\n",
            ",\n",
            "enverse\n",
            "segmenter\n",
            ",\n",
            "reconstruct\n",
            "original\n",
            "tokenisation\n",
            ".\n",
            "thing\n",
            "removing\n",
            "tokenisation\n",
            ".\n",
            "really\n",
            "would\n",
            "like\n",
            "reconstruct\n",
            "original\n",
            "one\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            "calculate\n",
            ",\n",
            "uh\n",
            ",\n",
            "SacreBLEU\n",
            "score\n",
            ".\n",
            "includes\n",
            "tokenisation\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yes\n",
            ",\n",
            "understand\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            ",\n",
            "so-\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "way\n",
            ".\n",
            "enverse\n",
            "segmenter\n",
            "still\n",
            "quite\n",
            "bad\n",
            "tool\n",
            ".\n",
            "Sometimes\n",
            "fails\n",
            "without\n",
            "without\n",
            "reason\n",
            "scripts\n",
            ".\n",
            "found\n",
            "[\n",
            "PERSON17\n",
            "]\n",
            ",\n",
            "run\n",
            "without\n",
            "tokenisation\n",
            "works\n",
            ".\n",
            "tokenisation\n",
            "fo\n",
            "like\n",
            "folds\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Oh\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "better\n",
            "reliable\n",
            "tokenisation\n",
            "'s\n",
            "100\n",
            "percent\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Robust\n",
            ".\n",
            "ca\n",
            "n't\n",
            "anything\n",
            ".\n",
            "Except\n",
            ",\n",
            "uh\n",
            ",\n",
            "re-implementing\n",
            "enverse\n",
            "segmenter\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            "..\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "scratch\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            ".\n",
            "n't\n",
            "source\n",
            "code\n",
            ".\n",
            "'s\n",
            "a-\n",
            ".\n",
            "think\n",
            "'s\n",
            "a-\n",
            ".\n",
            "comes\n",
            "[\n",
            "ORGANIZATION8\n",
            "]\n",
            ",\n",
            "usually\n",
            "comes\n",
            "without\n",
            "source\n",
            "code\n",
            ".\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Yes\n",
            ",\n",
            "paper\n",
            "least\n",
            ".\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yep\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "'s\n",
            "written\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            "okay\n",
            ".\n",
            ",\n",
            "well\n",
            ".\n",
            "Incorporate\n",
            "logic\n",
            "SLTF\n",
            ",\n",
            "uh\n",
            ",\n",
            "run\n",
            "enverse\n",
            "segmenter\n",
            ",\n",
            "uh\n",
            ",\n",
            "better\n",
            ",\n",
            "uh\n",
            ",\n",
            "include\n",
            ",\n",
            "uh\n",
            ",\n",
            "paranoic\n",
            "tests\n",
            ",\n",
            "uh\n",
            ",\n",
            "paranoid\n",
            "paranoid\n",
            "parano\n",
            "hm\n",
            "n't\n",
            "know\n",
            "right\n",
            "spelling\n",
            ".\n",
            "Parano\n",
            "think\n",
            "paranoid\n",
            "right-\n",
            ".\n",
            "Uh\n",
            ",\n",
            "test\n",
            "uh\n",
            ",\n",
            "amber\n",
            "segmenter\n",
            "output\n",
            ".\n",
            "Uh\n",
            ",\n",
            "yeah\n",
            ",\n",
            "okay\n",
            ".\n",
            "that's-\n",
            ".\n",
            "think\n",
            "'s\n",
            "like\n",
            "important\n",
            "exercise\n",
            "task\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ".\n",
            "Uh\n",
            ",\n",
            "uh\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "[\n",
            "PERSON13\n",
            "]\n",
            "need\n",
            ",\n",
            "uh\n",
            ",\n",
            "need\n",
            "implement\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "tokeniser\n",
            "first\n",
            "level\n",
            "work\n",
            ".\n",
            "use\n",
            ",\n",
            "uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "ca\n",
            "n't\n",
            "use\n",
            "uh\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "enverser\n",
            ",\n",
            "need\n",
            "run\n",
            "section\n",
            "SLT\n",
            "view\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "n't\n",
            "understand\n",
            ".\n",
            "Partly\n",
            ",\n",
            "partly\n",
            "network\n",
            "bad\n",
            ".\n",
            "Uh\n",
            ",\n",
            "maybe\n",
            "write\n",
            "[\n",
            "ORGANIZATION4\n",
            "]\n",
            "document\n",
            ".\n",
            "'ll\n",
            ",\n",
            "'ll\n",
            "get\n",
            "back\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "understood\n",
            "correctly\n",
            ",\n",
            "'re\n",
            "saying\n",
            ",\n",
            "uh\n",
            ",\n",
            "like\n",
            "organisation\n",
            "issue\n",
            "amber\n",
            "segmenter\n",
            ".\n",
            "sequence\n",
            "operations\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "see\n",
            "tokenising\n",
            "all-\n",
            ".\n",
            "Oh\n",
            ",\n",
            "n't\n",
            "use\n",
            ".\n",
            "uh\n",
            ".\n",
            ".\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "thinking\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "'s\n",
            "like\n",
            "'s\n",
            "design\n",
            "within\n",
            "SLTF\n",
            "proceed\n",
            "error\n",
            "rate\n",
            "words\n",
            ".\n",
            "n't\n",
            ".\n",
            "is-\n",
            ".\n",
            "'s\n",
            "'s\n",
            "like\n",
            "design\n",
            "decision\n",
            "SLTF\n",
            ".\n",
            "'s\n",
            "risky\n",
            "decision\n",
            ",\n",
            "tokeniations\n",
            "vary\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "um\n",
            ",\n",
            "think\n",
            "'s\n",
            "better\n",
            "operate\n",
            "level\n",
            "sentences\n",
            "tokenise\n",
            "purpose\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "LS\n",
            ",\n",
            "uh\n",
            ",\n",
            "run\n",
            "SL-\n",
            ".\n",
            "running\n",
            "amber\n",
            "segmenter\n",
            ".\n",
            ",\n",
            "untokenise\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "think\n",
            ".\n",
            "Like\n",
            "high\n",
            "level\n",
            "thinking\n",
            "SLTF\n",
            ".\n",
            "would\n",
            "like\n",
            "avoid\n",
            ",\n",
            "uh\n",
            ",\n",
            "forceful\n",
            "tokenisation\n",
            "quite\n",
            "sure\n",
            ".\n",
            "tokenisation\n",
            "peculiar\n",
            "thing\n",
            ".\n",
            "affects\n",
            "many\n",
            "many\n",
            "things\n",
            ".\n",
            "always\n",
            "particular\n",
            "purpose\n",
            "revert\n",
            "back\n",
            "original\n",
            "tokenisation\n",
            "much\n",
            "possible\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yeah\n",
            ".\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Oh\n",
            ".\n",
            "see\n",
            "editing\n",
            ".\n",
            "possible\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "using\n",
            "log-in\n",
            "?\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "'s\n",
            "probably\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ",\n",
            "yeah\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "see\n",
            "[\n",
            "PROJECT3\n",
            "]\n",
            "laptop\n",
            "it's-\n",
            ".\n",
            "logged-in\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "n't\n",
            "know\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "logged-in\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "let\n",
            "'s\n",
            ".\n",
            "Thank\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ".\n",
            "please\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ".\n",
            "Yes\n",
            ",\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            "'s\n",
            "question\n",
            ".\n",
            "Actually\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            "please\n",
            "write\n",
            "new\n",
            "assignment\n",
            ".\n",
            "tokenisation\n",
            "handled\n",
            ".\n",
            "write\n",
            "know\n",
            "record\n",
            ".\n",
            "Write\n",
            "'ve\n",
            "described\n",
            "know\n",
            "understood\n",
            "well\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "Okay\n",
            ".\n",
            "writing\n",
            "let\n",
            "'s\n",
            "move\n",
            "[\n",
            "PERSON17\n",
            "]\n",
            ".\n",
            "n't\n",
            "see\n",
            "[\n",
            "PERSON17\n",
            "]\n",
            "call\n",
            ".\n",
            "busy\n",
            "preparation\n",
            "exam\n",
            ".\n",
            "Reading\n",
            "shortening\n",
            ".\n",
            "Yes\n",
            ",\n",
            "um\n",
            ",\n",
            "um\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON17\n",
            "]\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON17\n",
            "]\n",
            "'s\n",
            "goal\n",
            "train\n",
            "[\n",
            "PROJECT1\n",
            "]\n",
            "systems\n",
            "reduce\n",
            "length\n",
            "output\n",
            ".\n",
            "like-\n",
            ".\n",
            "reading\n",
            "papers\n",
            ".\n",
            "'ll\n",
            "'ll\n",
            "start\n",
            "experimenting\n",
            "soon\n",
            ".\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            "suggesting\n",
            "something\n",
            "great\n",
            ".\n",
            "Okay\n",
            ",\n",
            "'s\n",
            "good\n",
            ".\n",
            "Uh\n",
            ".\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ",\n",
            "n't\n",
            "see\n",
            "writing\n",
            ".\n",
            "Please\n",
            "write\n",
            ",\n",
            "uh\n",
            ",\n",
            "write\n",
            "new\n",
            "ideal\n",
            "handling\n",
            "tokenisation\n",
            "SLTF\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            "move\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ".\n",
            "Uh\n",
            ",\n",
            "read\n",
            "ACL\n",
            "papers\n",
            ",\n",
            "wrote\n",
            ",\n",
            "uh\n",
            ",\n",
            "parts\n",
            "SLT\n",
            "deliverable\n",
            ".\n",
            "'s\n",
            "good\n",
            ".\n",
            "Uh\n",
            ",\n",
            "quick\n",
            "check\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            ".\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            "also\n",
            "written\n",
            "deliverable\n",
            "parts\n",
            ",\n",
            "right\n",
            "?\n",
            "ASR\n",
            "?\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "wrote\n",
            "ASR\n",
            "deliverables\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ",\n",
            "need\n",
            "review\n",
            "internal\n",
            "deadline\n",
            "6\n",
            "days\n",
            ".\n",
            "Uh\n",
            ",\n",
            ",\n",
            "hopefully\n",
            "get\n",
            "back\n",
            ".\n",
            "independently\n",
            "towards\n",
            "end\n",
            "week\n",
            "anything\n",
            "unclear\n",
            ".\n",
            "meet\n",
            "internal\n",
            "deadline\n",
            "8th\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "okay\n",
            ".\n",
            "Great\n",
            ".\n",
            "Uh\n",
            ".\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ",\n",
            "progress\n",
            "?\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Hmhm\n",
            ".\n",
            "Yes\n",
            ",\n",
            "reading\n",
            "papers\n",
            "found\n",
            "interesting\n",
            "tool\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "found\n",
            "'s\n",
            "possible\n",
            "measure\n",
            "speech\n",
            "rate\n",
            "cutting\n",
            "syllables\n",
            ".\n",
            "one\n",
            "tool\n",
            ".\n",
            "One\n",
            "patent\n",
            "tool\n",
            ",\n",
            "detect\n",
            "gender\n",
            "speaker\n",
            "speech\n",
            "rate\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "characteristics\n",
            ".\n",
            "try\n",
            "make\n",
            "dashboard\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "'s\n",
            "'s\n",
            "useful\n",
            "thing\n",
            ".\n",
            "Uh\n",
            ",\n",
            "later\n",
            "could\n",
            "even\n",
            "create\n",
            "models\n",
            "like-\n",
            ".\n",
            "recognise\n",
            "someone\n",
            "speaking\n",
            "fast\n",
            ",\n",
            "could\n",
            "use\n",
            "like\n",
            "harsher\n",
            "summarisation\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yes\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "could\n",
            "reducing\n",
            "reducing\n",
            "speech\n",
            "mole\n",
            "different\n",
            "model\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yes\n",
            ",\n",
            "also\n",
            "speech\n",
            "modes\n",
            ".\n",
            "Like\n",
            "whether\n",
            "angry\n",
            "normal\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "idea\n",
            "tool\n",
            "works\n",
            "practice\n",
            ".\n",
            "saw\n",
            "Gi\n",
            "GitHub\n",
            "buy\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "uh\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Actually\n",
            "'d\n",
            "like\n",
            "say\n",
            ".\n",
            "think\n",
            "um\n",
            "emotion\n",
            "detection\n",
            "used\n",
            "speech\n",
            ".\n",
            "basically\n",
            "bachelor\n",
            "'s\n",
            "thesis\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "worked\n",
            ".\n",
            "noticed\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "used\n",
            "MatLab\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ",\n",
            "please\n",
            "mention\n",
            "name\n",
            "tool\n",
            ".\n",
            "n't\n",
            "see\n",
            "yet\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "And-\n",
            ".\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ",\n",
            "since\n",
            "expertise\n",
            ",\n",
            "please\n",
            "check\n",
            "tool\n",
            ".\n",
            "before-\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "leave\n",
            ".\n",
            "Like\n",
            "d-\n",
            ".\n",
            "'m\n",
            "afraid\n",
            "that-\n",
            ".\n",
            "Maybe\n",
            "summer\n",
            ",\n",
            "summer\n",
            "could\n",
            "like\n",
            ",\n",
            "uh\n",
            ",\n",
            "um\n",
            ",\n",
            "integrate\n",
            "tool\n",
            "processes\n",
            ".\n",
            "even\n",
            "n't\n",
            "manage\n",
            "time\n",
            ",\n",
            "would\n",
            "good\n",
            "idea\n",
            "whether\n",
            "'s\n",
            "worth\n",
            "trying\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "definitely\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Uh\n",
            ",\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ".\n",
            "Uh\n",
            ",\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "review\n",
            ",\n",
            "uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "possibly\n",
            "integrate\n",
            "summer\n",
            ".\n",
            "obviously\n",
            "important\n",
            "clean\n",
            "um\n",
            "clean\n",
            "cruise\n",
            "control\n",
            ",\n",
            ",\n",
            "set-up\n",
            "set-up\n",
            "script\n",
            "summer\n",
            ".\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yes\n",
            ".\n",
            "think\n",
            "Czech\n",
            "segmenter\n",
            ".\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "mute\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "uses\n",
            "right\n",
            "context\n",
            "previous\n",
            "sentences\n",
            ".\n",
            "back\n",
            "end\n",
            "text\n",
            ".\n",
            "New\n",
            "entry\n",
            "point\n",
            "back\n",
            "end\n",
            "ASR\n",
            "basically\n",
            "text\n",
            ".\n",
            "segmenter\n",
            "receive\n",
            "1\n",
            "message\n",
            "emit\n",
            "2\n",
            "messages\n",
            ".\n",
            "connected\n",
            "on-line\n",
            "text\n",
            "flow\n",
            "works\n",
            ".\n",
            "call\n",
            "[\n",
            "PERSON5\n",
            "]\n",
            ".\n",
            "explained\n",
            "things\n",
            "easier\n",
            "fix\n",
            "bugs\n",
            ",\n",
            "found\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "video\n",
            "IWSLT\n",
            "June\n",
            "17th\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "quite\n",
            "soon\n",
            "well\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            "'m\n",
            "really\n",
            "afraid\n",
            "manage\n",
            ".\n",
            "anyone\n",
            "willing\n",
            ",\n",
            "uh\n",
            ",\n",
            "come\n",
            ".\n",
            "demo\n",
            "system\n",
            ".\n",
            "Uh\n",
            ",\n",
            "'ll\n",
            "'ll\n",
            "start\n",
            "pipeline\n",
            "everything\n",
            "anyone\n",
            "available\n",
            ",\n",
            "uh\n",
            ",\n",
            "demo\n",
            "system\n",
            ",\n",
            "then-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "mean\n",
            "video\n",
            "?\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "yeah\n",
            "video\n",
            ".\n",
            "mean\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "could\n",
            ",\n",
            "could\n",
            "interesting\n",
            ",\n",
            "yes\n",
            ".\n",
            "think\n",
            "'s\n",
            "a-\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "'m\n",
            "talking\n",
            "system\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "'s\n",
            "actually\n",
            "good\n",
            "idea\n",
            ".\n",
            "when-\n",
            ".\n",
            "n't\n",
            "know\n",
            "long\n",
            "presentation\n",
            ".\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ",\n",
            "please\n",
            "like\n",
            "propose\n",
            "timing\n",
            ".\n",
            "n't\n",
            "know\n",
            "whether\n",
            "like\n",
            "regular\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "se\n",
            "regular\n",
            "slides\n",
            "voice\n",
            "commentary\n",
            "?\n",
            "Uh\n",
            ",\n",
            ",\n",
            "uh\n",
            "could\n",
            "also\n",
            "indeed\n",
            "concatenate\n",
            "demo\n",
            "session\n",
            ",\n",
            "uh\n",
            ",\n",
            "whole\n",
            "system\n",
            ".\n",
            "think\n",
            "would\n",
            "pretty\n",
            "good\n",
            "illustrate\n",
            "runs\n",
            "live\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "15\n",
            "minutes\n",
            "presentation\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "en\n",
            "editioned\n",
            ".\n",
            "within\n",
            "15\n",
            "minutes\n",
            ",\n",
            "2\n",
            "minute\n",
            "3\n",
            "minute\n",
            "video\n",
            ",\n",
            "uh\n",
            ",\n",
            "Czech\n",
            "English\n",
            "languages\n",
            "think\n",
            "would\n",
            "good\n",
            ".\n",
            "it's-\n",
            ".\n",
            "'s\n",
            "different\n",
            ".\n",
            "Uh\n",
            ",\n",
            "um\n",
            ",\n",
            "actually\n",
            "hm\n",
            "hmhhm\n",
            ".\n",
            "think\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "actually\n",
            "demo\n",
            ",\n",
            "uh\n",
            ",\n",
            "task\n",
            "IWSLT.So\n",
            "Antrecorp\n",
            "horrible\n",
            "performing\n",
            "video\n",
            ",\n",
            "uh\n",
            ",\n",
            "like\n",
            "horrible\n",
            "performing\n",
            "speech\n",
            ".\n",
            "also\n",
            "uh\n",
            "um\n",
            "document\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "measuring\n",
            "word\n",
            "error\n",
            "rate\n",
            "SLTF\n",
            ".\n",
            "'s\n",
            "exactly\n",
            "test\n",
            "file\n",
            ".\n",
            "'s\n",
            "preparation\n",
            "demo\n",
            "session\n",
            ".\n",
            "Uh\n",
            ",\n",
            "is-\n",
            ".\n",
            "Could\n",
            "could\n",
            "recorded\n",
            "well\n",
            "used\n",
            "part\n",
            "IWSLT\n",
            "presentation\n",
            ".\n",
            "would\n",
            "go\n",
            "like\n",
            "2\n",
            "minutes\n",
            ",\n",
            "uh\n",
            ",\n",
            "3\n",
            "minutes\n",
            "15\n",
            "minute\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "talk\n",
            ".\n",
            "1\n",
            "minute\n",
            ",\n",
            "uh\n",
            ",\n",
            "one\n",
            "Antrecorp\n",
            "videos\n",
            ".\n",
            "Like\n",
            "horrible\n",
            "hard\n",
            "hard\n",
            "input\n",
            "reasonable\n",
            "input\n",
            ".\n",
            "Uh\n",
            ",\n",
            "works\n",
            "reasonably\n",
            "well\n",
            ".\n",
            ".\n",
            "'s\n",
            "'s\n",
            "great\n",
            "idea\n",
            ".\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "'s\n",
            "'s\n",
            "nothing\n",
            "different\n",
            "preparing\n",
            "videos\n",
            "demo\n",
            ".\n",
            "different\n",
            "thing\n",
            "would\n",
            "also\n",
            "run\n",
            "Antrecorp\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "speech\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "maybe\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ",\n",
            "scores\n",
            "files\n",
            ",\n",
            "right\n",
            "?\n",
            "pick\n",
            "file\n",
            "Antrecorp\n",
            ",\n",
            "uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "like\n",
            ",\n",
            "uh\n",
            ",\n",
            "works\n",
            "probably\n",
            "well\n",
            ".\n",
            "best\n",
            "Antrecorp\n",
            ".\n",
            "'s\n",
            "pretty\n",
            "bad\n",
            ".\n",
            "pretty\n",
            "bad\n",
            "believe\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            ",\n",
            "n't\n",
            "document\n",
            "scores\n",
            ".\n",
            "findings\n",
            "paper\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "see\n",
            ".\n",
            "Uhmm\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "validation\n",
            "error\n",
            ",\n",
            "2\n",
            "documents\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Umm\n",
            ",\n",
            "'s\n",
            "right\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "yeah\n",
            ",\n",
            "include\n",
            "one\n",
            "horrible\n",
            "Antrecorp\n",
            ",\n",
            "yeah\n",
            ".\n",
            "actually\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ".\n",
            "<\n",
            "laugh/\n",
            ">\n",
            "Provide\n",
            "ASR\n",
            ",\n",
            "uh\n",
            ",\n",
            "WR\n",
            "scores\n",
            "ehm\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            "'s\n",
            "runs\n",
            ",\n",
            "uh\n",
            ",\n",
            "Antrecorp\n",
            "files\n",
            ".\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            ",\n",
            "uh\n",
            ",\n",
            "?\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yes\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            ",\n",
            "sub-set\n",
            "lines\n",
            "output\n",
            "TSV\n",
            "file\n",
            ".\n",
            ",\n",
            "select\n",
            "appropriate\n",
            "lines\n",
            "output\n",
            "TSV\n",
            "file\n",
            ",\n",
            "uh\n",
            ",\n",
            "send\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Uh\n",
            ",\n",
            "yeah\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "would\n",
            "be-\n",
            ".\n",
            "would\n",
            "help\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "ASR\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Uh\n",
            ",\n",
            "think\n",
            "ASR\n",
            "much\n",
            "influencing\n",
            "translation\n",
            "quality\n",
            "'s\n",
            "okay\n",
            "go\n",
            "word\n",
            "error\n",
            "rate\n",
            ".\n",
            "'ll\n",
            "choose\n",
            "based\n",
            "word\n",
            "error\n",
            "rate\n",
            "'ll\n",
            "kind\n",
            "assume\n",
            "translation\n",
            "quality\n",
            ",\n",
            "uh\n",
            ",\n",
            "equal\n",
            "across\n",
            "different\n",
            "ASR\n",
            ",\n",
            "uh\n",
            ",\n",
            "qualities\n",
            ".\n",
            "ASR\n",
            "quality\n",
            "killing\n",
            "factor\n",
            ".\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Uh\n",
            ",\n",
            "send\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Okay\n",
            ".\n",
            "Thank\n",
            ".\n",
            "Uh\n",
            ",\n",
            "okay\n",
            ",\n",
            "now-\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "<\n",
            "other_noise/\n",
            ">\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            ",\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "mute\n",
            "?\n",
            "Yeah\n",
            ".\n",
            "some-\n",
            ".\n",
            ",\n",
            "think\n",
            "microphone\n",
            "sensitive\n",
            ".\n",
            "reason\n",
            ",\n",
            "uh\n",
            ",\n",
            "maybe\n",
            "check\n",
            "input\n",
            ",\n",
            "uh\n",
            ",\n",
            "like\n",
            "Windows\n",
            "setting\n",
            ".\n",
            "sorry\n",
            ",\n",
            "saying\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            "?\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Please\n",
            "check\n",
            "[\n",
            "PERSON19\n",
            "]\n",
            "section\n",
            ".\n",
            "Yeah\n",
            ",\n",
            "file\n",
            ",\n",
            "right\n",
            "?\n",
            "Yeah\n",
            "thi\n",
            "part\n",
            ",\n",
            "right\n",
            "?\n",
            "(\n",
            "PERSON19\n",
            ")\n",
            "Yes\n",
            ".\n",
            "Yes\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "'ll\n",
            "check\n",
            "lli\n",
            "like\n",
            "soon\n",
            ".\n",
            "Thank\n",
            ".\n",
            "Let\n",
            "'s\n",
            "let\n",
            "'s\n",
            "move\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            "quickly\n",
            ".\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            ",\n",
            "obviously\n",
            "working\n",
            "thesis\n",
            ".\n",
            "Uh\n",
            ",\n",
            "submitting\n",
            "thesis\n",
            ",\n",
            "uh\n",
            ",\n",
            ",\n",
            "uh\n",
            ",\n",
            "wrote\n",
            "parts-\n",
            ".\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            ",\n",
            "?\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "'m\n",
            ".\n",
            "I'm-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ",\n",
            "so-\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "basically\n",
            "mainly\n",
            "actually\n",
            "playing\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "server\n",
            ".\n",
            "n't\n",
            "know\n",
            "would\n",
            "useful\n",
            "us\n",
            ".\n",
            "Like\n",
            "emission\n",
            "translation\n",
            "segmentation\n",
            ",\n",
            "experiments\n",
            "'ve\n",
            "done\n",
            "like\n",
            "output\n",
            "gets\n",
            "updated\n",
            "much\n",
            "faster\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "like\n",
            "per\n",
            "second\n",
            ".\n",
            "'s\n",
            "like\n",
            "ten\n",
            "times\n",
            "per\n",
            "second\n",
            ",\n",
            "yes\n",
            ",\n",
            "'s\n",
            "updating\n",
            "much\n",
            "faster\n",
            "uh\n",
            "second\n",
            "thing\n",
            "put\n",
            "get\n",
            "board\n",
            "level\n",
            "time\n",
            "stamps\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "word\n",
            "current\n",
            "hypothesis\n",
            ",\n",
            "uh\n",
            ",\n",
            "because-\n",
            ".\n",
            "Right\n",
            ",\n",
            "beginning\n",
            "whole\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "current\n",
            "path\n",
            "end\n",
            "time\n",
            "stamps\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "like\n",
            "additional\n",
            "functionality\n",
            "output\n",
            "like\n",
            "word\n",
            "level\n",
            "time\n",
            "stamps\n",
            ".\n",
            "'m\n",
            "'m\n",
            "sure\n",
            "would\n",
            "helpful\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "would\n",
            "like\n",
            "set-up\n",
            "rely\n",
            "mediator\n",
            ".\n",
            "TCP\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "server\n",
            "useful\n",
            "mediator\n",
            "set-up\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "server\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "maybe\n",
            "would\n",
            "good\n",
            ",\n",
            "uh\n",
            ",\n",
            "get\n",
            "touch\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            ",\n",
            "uh\n",
            ",\n",
            "features\n",
            "found\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "server\n",
            ",\n",
            "TCP\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "server\n",
            "also\n",
            "made\n",
            "available\n",
            "'s\n",
            "hard\n",
            "like\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "worker\n",
            ".\n",
            "Would\n",
            "make\n",
            "sence\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            "?\n",
            "n't\n",
            "know\n",
            ",\n",
            "uh\n",
            ",\n",
            "well\n",
            "please\n",
            "talk\n",
            "directly\n",
            ".\n",
            "[\n",
            "PERSON10\n",
            "]\n",
            "[\n",
            "PERSON18\n",
            "]\n",
            "talk\n",
            "figure\n",
            "much\n",
            "work\n",
            "would\n",
            ",\n",
            "uh\n",
            ",\n",
            "would\n",
            "involve\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "changes\n",
            "'ve\n",
            "'ve\n",
            "put\n",
            "directly\n",
            "like\n",
            "binary\n",
            "TCP\n",
            "server\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            ",\n",
            "maybe\n",
            "changes\n",
            "could\n",
            "done\n",
            "within\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "worker\n",
            ".\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "Yes\n",
            ".\n",
            "'m\n",
            "'m\n",
            "'m\n",
            "actually\n",
            "sure\n",
            "ho\n",
            "worker\n",
            "exactly\n",
            "works\n",
            ".\n",
            "But-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Uh\n",
            ",\n",
            "talk\n",
            "to-\n",
            ".\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "wor\n",
            "worker\n",
            "resends\n",
            "receives\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "it-\n",
            ".\n",
            "think\n",
            "uses\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "binary\n",
            ",\n",
            "TCP\n",
            "server\n",
            ",\n",
            "right\n",
            "?\n",
            "(\n",
            "PERSON18\n",
            ")\n",
            "Yes\n",
            ".\n",
            "gave\n",
            "us\n",
            "shell\n",
            "scripts\n",
            "starts\n",
            "TCP\n",
            "server\n",
            "ports\n",
            "connects\n",
            "ports\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Yes\n",
            ",\n",
            "changes\n",
            "done\n",
            "inside\n",
            "binary\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "easy\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "like\n",
            "option\n",
            ",\n",
            "find\n",
            "useful\n",
            "space\n",
            "get\n",
            "word\n",
            "level\n",
            "time\n",
            "stamps\n",
            "real\n",
            "time\n",
            "recording\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yes\n",
            ",\n",
            "would\n",
            "like\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "much\n",
            "faster\n",
            "updating\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "w\n",
            "would\n",
            "like\n",
            "word\n",
            "level\n",
            "time\n",
            "stamps\n",
            ",\n",
            "uh\n",
            ",\n",
            "log\n",
            "files\n",
            ".\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "actually\n",
            ".\n",
            "log\n",
            "files\n",
            "verbose\n",
            ".\n",
            "'d\n",
            "like\n",
            ",\n",
            "uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "well\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            ".\n",
            "Make\n",
            "sure\n",
            "log\n",
            "files\n",
            "include\n",
            "word\n",
            "level\n",
            "time\n",
            "stamps\n",
            ".\n",
            "Okay\n",
            ",\n",
            "thank\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "when-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "level\n",
            "time\n",
            "stamps\n",
            "ASR\n",
            "output\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "ASR\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "Mhm\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            "real\n",
            "time\n",
            "recognition\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Mhm\n",
            ",\n",
            "okay\n",
            "okay\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            ",\n",
            "useful\n",
            "SLTF\n",
            "evaluation\n",
            "simultaneity\n",
            "speech\n",
            ".\n",
            "know\n",
            "word\n",
            ",\n",
            "recognised\n",
            ",\n",
            "link\n",
            ",\n",
            "uh\n",
            ",\n",
            "score\n",
            "golden\n",
            "transcript\n",
            ".\n",
            "(\n",
            "PERSON6\n",
            ")\n",
            "Okay\n",
            ",\n",
            "okay\n",
            ".\n",
            "(\n",
            "PERSON10\n",
            ")\n",
            ".\n",
            "like\n",
            "copy\n",
            "original\n",
            "TCP\n",
            "server\n",
            "'ve\n",
            "modified\n",
            "like\n",
            "launch\n",
            "[\n",
            "PROJECT2\n",
            "]\n",
            "script\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "[\n",
            "deleted\n",
            "18\n",
            "lines\n",
            "]\n",
            "[\n",
            "PERSON3\n",
            "]\n",
            "progress\n",
            "?\n",
            "?\n",
            ",\n",
            "'m\n",
            "sure\n",
            ".\n",
            "n't\n",
            "hear\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "Hello\n",
            "?\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            "works\n",
            ".\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "Okay\n",
            ".\n",
            "Hi\n",
            ",\n",
            "collected\n",
            ",\n",
            "uh\n",
            ",\n",
            "Georgian\n",
            "name\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "gave\n",
            ".\n",
            "last\n",
            "meeting\n",
            "discussed\n",
            "like\n",
            "remaining\n",
            "languages\n",
            "like\n",
            "Georgian\n",
            "Hebrew\n",
            "uh-\n",
            ".\n",
            "another\n",
            "language\n",
            ",\n",
            "uh-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Icelandic\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "Icelandic\n",
            ",\n",
            "yes\n",
            "collected\n",
            "upload\n",
            "today\n",
            ".\n",
            "<\n",
            "unintelligible/\n",
            ">\n",
            "data\n",
            ".\n",
            "files\n",
            "'ve\n",
            "already\n",
            "sent\n",
            "Russian\n",
            ",\n",
            "Serbian\n",
            "name\n",
            ".\n",
            "Croatian\n",
            ".\n",
            "[\n",
            "PERSON14\n",
            "]\n",
            "[\n",
            "PERSON14\n",
            "]\n",
            "file\n",
            "name\n",
            "yeah\n",
            "Danish\n",
            ".\n",
            "Yeah\n",
            "today\n",
            "sent\n",
            "Danish\n",
            "file\n",
            "[\n",
            "PERSON14\n",
            "]\n",
            "validation\n",
            ".\n",
            "discussed-\n",
            ".\n",
            "last\n",
            "meeting\n",
            "discussed\n",
            ",\n",
            "okay\n",
            ",\n",
            "like\n",
            "like\n",
            "similar\n",
            "closer\n",
            "languages\n",
            "validation\n",
            ".\n",
            "conversation\n",
            "[\n",
            "PERSON14\n",
            "]\n",
            "agreed\n",
            "Danish\n",
            "Dutch\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "sent\n",
            "Danish\n",
            "remaining\n",
            "Russian\n",
            "text\n",
            "data\n",
            "'ve\n",
            "sent\n",
            "sent\n",
            "[\n",
            "PERSON1\n",
            "]\n",
            "validation\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            "okay\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "n't\n",
            "know\n",
            "many\n",
            "is-\n",
            ".\n",
            "okay\n",
            ",\n",
            "working-\n",
            ".\n",
            "received\n",
            ",\n",
            "15\n",
            "20\n",
            "minute\n",
            "received\n",
            "[\n",
            "PERSON7\n",
            "]\n",
            "email\n",
            "remaining\n",
            "annotators\n",
            "send\n",
            "annotators\n",
            "files\n",
            ".\n",
            "Uh\n",
            ",\n",
            "regarding\n",
            "Norwegian\n",
            "n't\n",
            "receive\n",
            "response\n",
            "e\n",
            "[\n",
            "PERSON4\n",
            "]\n",
            "[\n",
            "PERSON4\n",
            "]\n",
            "sorry\n",
            "n't\n",
            "know\n",
            "actually\n",
            "[\n",
            "PERSON4\n",
            "]\n",
            "[\n",
            "PERSON4\n",
            "]\n",
            ".\n",
            "response\n",
            ".\n",
            "sent\n",
            "30th\n",
            "April\n",
            "3000\n",
            "sentences\n",
            "n't\n",
            "receive\n",
            "response\n",
            ".\n",
            "remain\n",
            ".\n",
            "sent\n",
            "2\n",
            "reminders\n",
            "n't\n",
            "know\n",
            ".\n",
            "Maybe\n",
            "is-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Okay\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "Busy\n",
            "another\n",
            "work\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Okay\n",
            ".\n",
            "'s\n",
            "'s\n",
            "bad\n",
            ".\n",
            "Well\n",
            "obviously\n",
            "happen\n",
            ".\n",
            "like\n",
            "come-up\n",
            "backup\n",
            "solutions\n",
            "ask\n",
            "people\n",
            ",\n",
            "uh-\n",
            ".\n",
            "Yes\n",
            ",\n",
            "'s\n",
            "'s\n",
            "possible\n",
            "like-\n",
            ".\n",
            ",\n",
            "uh\n",
            ",\n",
            "providing\n",
            "input\n",
            "quickly\n",
            "enough\n",
            ",\n",
            "uh\n",
            ",\n",
            "got\n",
            "busy\n",
            "things\n",
            ".\n",
            "'s\n",
            "'s\n",
            "possible\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            ",\n",
            "maybe\n",
            "'s\n",
            "possible\n",
            ".\n",
            "have-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "n't\n",
            "know-\n",
            ".\n",
            "(\n",
            "PERSON3\n",
            ")\n",
            "'ve\n",
            "asked\n",
            "[\n",
            "PERSON16\n",
            "]\n",
            "annotator\n",
            ".\n",
            "'ve\n",
            "already\n",
            "shared\n",
            ",\n",
            "uh\n",
            ",\n",
            "annotator\n",
            "list\n",
            "created\n",
            "[\n",
            "PERSON6\n",
            "]\n",
            "think\n",
            ".\n",
            "[\n",
            "ORGANIZATION5\n",
            "]\n",
            "language\n",
            "map\n",
            ".\n",
            "Uh\n",
            ",\n",
            "uh\n",
            ",\n",
            "already\n",
            "shared\n",
            "agree\n",
            "unle\n",
            "annotator\n",
            "send\n",
            "files\n",
            "validations\n",
            ".\n",
            "validation\n",
            "done\n",
            ",\n",
            "merge\n",
            "like\n",
            "tester-\n",
            ".\n",
            "create\n",
            "new\n",
            "tester\n",
            "directory\n",
            "put\n",
            "multilingual\n",
            "parallel\n",
            "data\n",
            "equivalent\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ".\n",
            "Otherwise\n",
            "create-\n",
            ".\n",
            "create\n",
            "problem\n",
            "like\n",
            "count\n",
            "sentence\n",
            "alignment\n",
            "'s\n",
            "different\n",
            "status\n",
            "so-\n",
            ".\n",
            "(\n",
            "PERSON13\n",
            ")\n",
            "Yeah\n",
            ",\n",
            "yeah\n",
            ".\n",
            "Total word count after stopword removal: 4850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ndhgy7keX6gw",
        "outputId": "a743b8b6-a9bd-499e-a9c7-abcce6875faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "# Step 1: Load a BERT-based model for sentence embeddings\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # A lightweight BERT-based model\n",
        "\n",
        "# Step 2: Generate sentence embeddings for the pre-tokenized sentences\n",
        "sentence_embeddings = model.encode(filtered_words)\n",
        "\n",
        "# Step 3: Use Agglomerative Clustering to segment the sentences based on similarity\n",
        "n_clusters = 3  # Adjust the number of clusters as necessary\n",
        "clustering_model = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "cluster_assignment = clustering_model.fit_predict(sentence_embeddings)\n",
        "\n",
        "# Step 4: Group sentences into clusters based on the cluster assignment\n",
        "segmented_text = [[] for _ in range(n_clusters)]\n",
        "for sentence, cluster_id in zip(sentences, cluster_assignment):\n",
        "    segmented_text[cluster_id].append(sentence)\n",
        "\n",
        "# Step 5: Print the segmented topics and their word counts\n",
        "print(\"\\nSegmented Topics with Word Counts:\")\n",
        "for i, segment in enumerate(segmented_text):\n",
        "    # Concatenate the sentences in each segment to form a text block\n",
        "    segment_text = ' '.join(segment)\n",
        "\n",
        "    # Count the words in each segment by splitting on spaces (stopwords already removed)\n",
        "    word_count = len(segment_text.split())\n",
        "\n",
        "    print(f\"Segment {i + 1} (Word Count: {word_count}):\")\n",
        "    for sentence in segment:\n",
        "        print(f\"  {sentence}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPqlj9e7Spll",
        "outputId": "a307d7ab-7354-4c86-f94a-9f50f8705caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmented Topics with Word Counts:\n",
            "Segment 1 (Word Count: 3100):\n",
            "  (PERSON13) Hi.\n",
            "  Hello [PERSON19].\n",
            "  (PERSON6) Hi everyone.\n",
            "  (PERSON13) Yeah, great.\n",
            "  So, yeah.\n",
            "  Uh, I I see that people have written up ehm what they did.\n",
            "  (PERSON13) Yep, that's great.\n",
            "  Yes, so that's that's re re record.\n",
            "  What you did.\n",
            "  So, uh, [PERSON13], uh I am busy, uh, with the IW SLT, uh, write-up.\n",
            "  Now busy with interviewing people people to uh to replace those who are em moving forward <laugh/> so to say.\n",
            "  So there is number of colleagues on projects that I am supervising, uh, that who are going for studies abroad and other things.\n",
            "  Then we need to focus on the ladder climbing, uh, which is building uh, uh, [PROJECT3] test set plus, uh, regularly, uh, testing on it.\n",
            "  Yeah.\n",
            "  [PROJECT3] deliverables.\n",
            "  So, I'll try to provide the links-.\n",
            "  And yeah.\n",
            "  So [PERSON6] you are the first on the list.\n",
            "  Ehm, ehm, so please briefly update what what you have been working on.\n",
            "  (PERSON6) <other_noise/>\n",
            "So, luckily.\n",
            "  <laugh/>\n",
            "Not luckily but this week I had like quite less tasks to do.\n",
            "  So first I calculated the word error rate on Czech transcripts using that three versions of, uh, Czech ASR which [PERSON10] created.\n",
            "  And so yesterday [PERSON10] told me that they were, uh, and the golden transcript and its corresponding video there were there were huge huge mismatch.\n",
            "  And I <unintelligible/> and he said to update me.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON6) And lastly yeah I think I did-.\n",
            "  Uh, it was the input in the [PROJECT3] deliverable of for the punctuator and through caser.\n",
            "  <unintelligible/>\n",
            "(PERSON13) Mhm, yeah.\n",
            "  (PERSON6) And I don't have-.\n",
            "  I think that apart from the testing sessions to do this week so I am waiting for new tasks.\n",
            "  (PERSON13) Yeah, so.\n",
            "  So the word error rate, there is also the English, uh, transcripts?\n",
            "  Ehm, and also we should have from [PERSON9] the German one, right?\n",
            "  So.\n",
            "  (PERSON6) Yeah, yeah, yeah.\n",
            "  So there is always the English speaker and then the Czech speaker who repeats the same content.\n",
            "  And [PERSON7] has split the video and while the English part should be more reliable, uh, the Czech part has been done simply by using the other ends.\n",
            "  Like the end of English and the beginning of the next English segment.\n",
            "  So that's why I'm not surprised that the Czech video is, uh, imprecise in timing.\n",
            "  So, uh, that is something that, yeah.\n",
            "  [PERSON10], can you maybe tell us more details about that?\n",
            "  Well, I just like listened to the audio and followed the talk transcript <other_noise/>\n",
            "and it was completely off.\n",
            "  I think it is-.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON10) Yeah, yeah.\n",
            "  The transcription is for the completely different audio than it's in the subdirectory.\n",
            "  (PERSON6) Mhm.\n",
            "  (PERSON13) Oh, so then someone must have like messed it up.\n",
            "  (PERSON10) Yeah, yeah, I I may maybe it's just like uh.\n",
            "  Maybe the files are just switched between the subdirectories?\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON10) I I haven't checked but-.\n",
            "  (PERSON13) Yeah, so [PERSON10] can you coul could you do this check?\n",
            "  It should not be hard\n",
            "Like try listening to all the files that are within this demo for [PERSON15], uh, and try to locate the correct file, the appropriate files.\n",
            "  But we should have, we should have the transcripts ready for all of those.\n",
            "  So we should be able to, uh, to evaluate it.\n",
            "  And also for the English ones we have the translations.\n",
            "  But also the machine translation quality or at the SLT even.\n",
            "  Uh, with the translation quality into German and Czech.\n",
            "  Both are available.\n",
            "  (PERSON6) Okay.\n",
            "  (PERSON13) We have these files ready.\n",
            "  (PERSON6) And so these, for like a German audio and English [PROJECT1].\n",
            "  (PERSON13) English, uh, input for English sound.\n",
            "  We have the golden English transcript, so you can check the ASR.\n",
            "  And we also have the translation into Czech and into German.\n",
            "  So you can also evaluate directly the translation quality, uh, of that.\n",
            "  (PERSON6) Okay, yeah.\n",
            "  (PERSON13) Yeah, so this is, this is an important, uh, task, uh, to do, uh wr also for German and English audios.\n",
            "  And another to do, uh, bleu or SLTF, uh, for, uh, German and Czech translations of, uh, English.\n",
            "  (PERSON6) ASR.\n",
            "  Yeah.\n",
            "  (PERSON6) Yeah.\n",
            "  (PERSON13) Uh, okay.\n",
            "  And with the, with this testing sessions uhm uh, so when will we have some successful or like reliable, uh, testing sessions?\n",
            "  Did [PERSON11] say or [PERSON12] say anything about the the new date?\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON6) So, like they didn't work at all.\n",
            "  And today [PERSON12] was working in the morning and I don't have any updates from [PERSON12] what\n",
            "the status of-.\n",
            "  But yeah, to today [PERSON12] was doing the fixings of the segmenter.\n",
            "  (PERSON13) Yeah.\n",
            "  Yeah, please also check the timing.\n",
            "  <laugh/>\n",
            "Including the segmenter.\n",
            "  Yeah.\n",
            "  So then, uh, yeah the-.\n",
            "  So I need to review that.\n",
            "  Yeah, that's good.\n",
            "  (PERSON6) Yeah, yeah.\n",
            "  Okay.\n",
            "  (PERSON13) And I had another idea and that was, yeah.\n",
            "  Uh, so that's, uh, um my question like backing up of our systems into net data [PROJECT3] systems.\n",
            "  (PERSON13) Yeah, including you.\n",
            "  That have been created.\n",
            "  So imagine that it's us who are without connectivity.\n",
            "  We need to pick someone, uh, who will be happy to run systems for ourselves.\n",
            "  So, so if you could-.\n",
            "  When you are in touch with these, uh, please talk to them and, uh, let me know.\n",
            "  Because on Thursday we have [PROJECT3] call and for that call I would actually like to know if we have a substitute site for each of our systems.\n",
            "  Yeah, thank you.\n",
            "  So there is lot of work to do on the evaluation.\n",
            "  (PERSON6) Yeah.\n",
            "  (PERSON13) And the I would like really very soon to see some numbers that we trust.\n",
            "  So obviously there is this problem with the Czech.\n",
            "  And now hopefully there will not be other problems with with the other thing, uh, other things.\n",
            "  And this-.\n",
            "  (PERSON6) I mean, he must have received some audio by which, after which he he has written those transcripts manually.\n",
            "  But the the audio input, uh, that the transcript corresponds to, that is the audio which has been released as the IWSLT test set.\n",
            "  So anyone can, uh, can easily check the IWSLT test set.\n",
            "  (PERSON6) Mhm.\n",
            "  Yeah.\n",
            "  Yeah.\n",
            "  (PERSON13) Yeah.\n",
            "  Okay\n",
            "So thanks, thanks for this.\n",
            "  Now for [PERSON19].\n",
            "  Uh, and these days, uh, what you are working on [PERSON19]?\n",
            "  (PERSON19) Uh yes, I can hear you.\n",
            "  Hello.\n",
            "  Uh.\n",
            "  [PERSON13].\n",
            "  (PERSON19) Can you hear me?\n",
            "  (PERSON13) Yes.\n",
            "  (PERSON19) This week I have been working on IWSLT submissions files again.\n",
            "  (PERSON19) But there is a problem about calculating the bleu score.\n",
            "  (PERSON19) As I've sent you in the email.\n",
            "  I think we need a good tokeniser.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON19) And another problem <unintelligible/> calculating the bleu score.\n",
            "  I have used the NLTK model for calculating the scores.\n",
            "  So we have discussed the NLTK versus SacreBLEU with [PERSON2].\n",
            "  And we do not trust the NLTK bleu score at all.\n",
            "  Because it is probably not tokenising and also accidentally it is-.\n",
            "  It drops to 0.\n",
            "  It can be both, inflated and and too small.\n",
            "  We trust only Sacre-BLEU.\n",
            "  And I think they're like-.\n",
            "  Please write.\n",
            "  Oh you have-.\n",
            "  Uh,\n",
            "Well, SLTF thank you for the organisation.\n",
            "  See you later.\n",
            "  Hmm.\n",
            "  <unintelligible/>\n",
            "Yes, so es essentially to answer your question in the email.\n",
            "  We have to switch to and we have for  the IWSLT.\n",
            "  We have to switch to SacreBLEU and SacreBLEU does its own tokenisation before scoring.\n",
            "  Let's let's simply forget NLTK bleu score.\n",
            "  (PERSON18) Yes, but-.\n",
            "  Let's just forget it.\n",
            "  Let's let's just use SacreBLEU.\n",
            "  (PERSON18) I I have one comment about it.\n",
            "  (PERSON18) You sh should use tokeniser before enverse segmenter.\n",
            "  Yeah.\n",
            "  Because it can rely on the on the dots and commas and question marks and so on.\n",
            "  (PERSON18) And it's it's using the Moses seg tokeniser and detokeniser.\n",
            "  Do you understand?\n",
            "  (PERSON19) Yeah.\n",
            "  It is much better to tokenise it before enverse segmenter and then maybe undo the tokenisation again.\n",
            "  Which is not the same thing as removing tokenisation.\n",
            "  I really would like to reconstruct the original one.\n",
            "  And then, uh, uh calculate, uh, the SacreBLEU score.\n",
            "  (PERSON18) Yeah.\n",
            "  (PERSON13) Okay, so-.\n",
            "  The enverse segmenter is still quite bad tool.\n",
            "  Sometimes it fails without without reason on some scripts.\n",
            "  (PERSON13) Oh.\n",
            "  When it comes from [ORGANIZATION8], it usually comes without a source code.\n",
            "  So.\n",
            "  (PERSON18) Yeah.\n",
            "  (PERSON13) Yep.\n",
            "  <laugh/>\n",
            "(PERSON18) It's written.\n",
            "  So, well.\n",
            "  Parano I think paranoid is the right-.\n",
            "  Uh, yeah, okay.\n",
            "  I think that's a a like an important exercise for task for you [PERSON19].\n",
            "  Uh, and uh.\n",
            "  (PERSON19) But [PERSON13] you need to, uh, you need to implement <unintelligible/> that <unintelligible/> or that this tokeniser in the first level of our work.\n",
            "  Yeah.\n",
            "  There is no, uh, uh, it's like it's your design within SLTF that you proceed with an error rate of words.\n",
            "  And it's a risky decision, because the tokeniations vary.\n",
            "  Such as, uh, LS, uh, run SL-.\n",
            "  Such as running amber segmenter.\n",
            "  So, uh, think again.\n",
            "  Like do high level thinking about SLTF.\n",
            "  And I would like you to avoid, uh, this forceful tokenisation when we are not quite sure about it.\n",
            "  So tokenisation is a peculiar thing.\n",
            "  And you should always do it for a particular purpose and then revert back to the original tokenisation as as much as possible.\n",
            "  (PERSON19) Yeah.\n",
            "  So.\n",
            "  (PERSON13) Oh.\n",
            "  I see myself editing this.\n",
            "  How is that possible.\n",
            "  <laugh/>\n",
            "<unintelligible/>\n",
            "<laugh/>\n",
            "That's probably [PERSON6], yeah.\n",
            "  You have been logged-in.\n",
            "  <laugh/>\n",
            "(PERSON6) I don't know.\n",
            "  Okay.\n",
            "  (PERSON6) I have logged-in now.\n",
            "  So let's.\n",
            "  Thank you [PERSON19].\n",
            "  So please, uh, uh, uh.\n",
            "  Yes, that was [PERSON19]'s question.\n",
            "  How the tokenisation should be handled.\n",
            "  Write down what I've just described so I know that that you understood as well.\n",
            "  Yeah.\n",
            "  Okay.\n",
            "  And while you are writing let's move to [PERSON17].\n",
            "  I don't see [PERSON17] in the call.\n",
            "  So he is busy with preparation for exam.\n",
            "  Yes, his um, um.\n",
            "  So,uh, [PERSON17] uh, [PERSON17]'s goal now is to train [PROJECT1] systems so that they reduce the length of the output.\n",
            "  And [PERSON18] is suggesting something great.\n",
            "  Please write down, uh, what write down the the new ideal handling of tokenisation in SLTF.\n",
            "  (PERSON13) Yeah.\n",
            "  (PERSON19) I will.\n",
            "  (PERSON13) Okay\n",
            "And we can move to [PERSON18].\n",
            "  Uh, so you have read some ACL papers, wrote, uh, parts of the SLT deliverable.\n",
            "  That's good.\n",
            "  Uh, just a quick check with [PERSON10].\n",
            "  The ASR?\n",
            "  (PERSON13) Yes, so I need to review these and the internal deadline is in 6 days from now.\n",
            "  Uh, so, hopefully I will get back to all of you.\n",
            "  To each of you independently towards the end of the week if there is anything unclear.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON18) I found out that it's possible to measure out the speech rate by cutting the syllables.\n",
            "  One patent tool, which can detect the gender of speaker and the speech rate.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON18) And some other characteristics.\n",
            "  So we can try it and make a dashboard out of it.\n",
            "  If we if we recognise that someone is speaking too fast, we could use like a harsher summarisation.\n",
            "  (PERSON18) Yes.\n",
            "  (PERSON18) Yes, and there was also speech modes.\n",
            "  Like whether it was angry or normal and so on.\n",
            "  (PERSON13) Mhm.\n",
            "  I I I I saw it only in Gi GitHub and I buy it.\n",
            "  (PERSON6) Actually I'd like to say here.\n",
            "  So this is about a I think um emotion detection used in speech.\n",
            "  So this is basically my bachelor's thesis.\n",
            "  <laugh/>\n",
            "(PERSON18) Yeah.\n",
            "  And I noticed <unintelligible/>\n",
            "And I just used MatLab.\n",
            "  (PERSON13) Yeah.\n",
            "  So [PERSON18], please mention the the name of the tool here.\n",
            "  Because I don't see that yet.\n",
            "  (PERSON18) Okay.\n",
            "  (PERSON13) Yeah.\n",
            "  And [PERSON6], since you have the expertise in this, please check the tool.\n",
            "  So before-.\n",
            "  (PERSON6) Yeah, yeah.\n",
            "  (PERSON13) Be before you leave.\n",
            "  Like I d-.\n",
            "  I'm afraid that-.\n",
            "  Maybe over the summer, over the summer you could like, uh, um, integrate this tool into into our processes.\n",
            "  But even if you don't manage this in time, it would be good to have to have an idea whether it's worth trying that.\n",
            "  (PERSON13) Uh and then, and then to do for [PERSON6].\n",
            "  Uh, to do for [PERSON6], uh, [PERSON6] to review it, uh, and, uh, possibly integrate it over the summer.\n",
            "  Yeah.\n",
            "  (PERSON18) Yes.\n",
            "  And I I think that the Czech segmenter.\n",
            "  (PERSON13) Yeah.\n",
            "  And I had a call with [PERSON5].\n",
            "  (PERSON13) Yeah, which is quite soon as well.\n",
            "  (PERSON6) Yeah so I'm really afraid to manage.\n",
            "  And demo the system.\n",
            "  Uh, I'll I'll start the pipeline everything and if anyone is available to, uh, demo the  system, then-.\n",
            "  (PERSON13) You mean for the video?\n",
            "  (PERSON6) Yeah, yeah, yeah for the video.\n",
            "  (PERSON13) That could be, that could be interesting, yes.\n",
            "  That's actually a good idea.\n",
            "  So when-.\n",
            "  (PERSON13) Mhm.\n",
            "  So if we en editioned.\n",
            "  So it's-.\n",
            "  That's exactly the test file.\n",
            "  So that's the preparation for the demo session.\n",
            "  Uh, is-.\n",
            "  Like the horrible hard hard input and some reasonable input.\n",
            "  That's that's a great idea.\n",
            "  All of all of them are pretty bad I believe.\n",
            "  (PERSON18) No, I don't have the document scores.\n",
            "  Yeah, yeah, yeah.\n",
            "  So, uh, yeah, will include one horrible Antrecorp, yeah.\n",
            "  So this is actually a to do for, uh, [PERSON19].\n",
            "  <laugh/>\n",
            "Provide ASR, uh, so WR scores for ehm [PERSON18]'s runs, uh, of all Antrecorp files.\n",
            "  So [PERSON19], uh, are you here?\n",
            "  (PERSON19) Yes.\n",
            "  So, if you can select the appropriate lines of that output TSV file and, uh, and then send it to, uh, [PERSON18] and me.\n",
            "  (PERSON13) That that would be-.\n",
            "  That would help.\n",
            "  (PERSON19) Only for ASR?\n",
            "  (PERSON13) Uh, I think that the ASR is so much influencing the translation quality that it's okay to go with AS the word error rate.\n",
            "  (PERSON19) Uh, I I will send you for <unintelligible/>.\n",
            "  (PERSON13) Yeah, yeah.\n",
            "  Okay.\n",
            "  <unintelligible/>\n",
            "<other_noise/>\n",
            "(PERSON13) Can you, [PERSON6] can you can you mute yourself?\n",
            "  There is some-.\n",
            "  So sorry, what were you saying [PERSON19]?\n",
            "  (PERSON19) Please check my [PERSON19] section.\n",
            "  Yeah, so this this file, right?\n",
            "  Yeah thi this part, right?\n",
            "  Yes.\n",
            "  (PERSON13) Yeah, I'll check that in a lli like soon.\n",
            "  Thank you.\n",
            "  Let's let's move to [PERSON10] quickly.\n",
            "  Uh, submitting thesis, uh, and then, uh, wrote a parts-.\n",
            "  So [PERSON10], are you here?\n",
            "  (PERSON10) Yeah, yeah, yeah.\n",
            "  I'm-.\n",
            "  (PERSON10) So so basically I I have been mainly actually playing with the [PROJECT2] server.\n",
            "  So I don't know if it would be useful for us.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON10) So now it was like once per second.\n",
            "  And now it's like ten times per second, so yes, so it's updating much faster and uh second thing that I put there is that we can get a board level time stamps.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON10) For each of the word in the current hypothesis, uh, because-.\n",
            "  Right now, it was just the beginning of the whole <unintelligible/> current path and the end time stamps.\n",
            "  But now, uh, there is like some additional functionality that it can output like the word level time stamps.\n",
            "  So I'm I'm not sure if if it would be helpful.\n",
            "  (PERSON13) Okay, so maybe the same changes could be done within the [PROJECT2] worker.\n",
            "  So.\n",
            "  (PERSON10) Mhm.\n",
            "  Yes.\n",
            "  I'm I'm I'm not actually sure ho how the worker exactly works.\n",
            "  Uh, talk to-.\n",
            "  (PERSON18) The wor the worker only resends what it receives from [PROJECT2].\n",
            "  (PERSON10) So it-.\n",
            "  (PERSON18) Yes.\n",
            "  You gave us some shell scripts which starts the TCP server on some ports and it connects to this ports.\n",
            "  (PERSON10) Yes, so so these changes I have done are inside this binary.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON10) So there is like the option if we, if we find useful space for it then we can get word level time stamps for the real time recording.\n",
            "  (PERSON13) Yes, I would like to have them.\n",
            "  (PERSON10) <unintelligible/> much faster updating the <unintelligible/>\n",
            "(PERSON13) Yeah, I w I would like to have the word level time stamps in the, uh, in the log files.\n",
            "  So this is for [PERSON6] actually.\n",
            "  So that the log files are very verbose.\n",
            "  So, uh, to do for [PERSON6].\n",
            "  (PERSON6) So when-.\n",
            "  (PERSON6) So what level time stamps of the ASR output of [PROJECT2] ASR?\n",
            "  (PERSON13) Yeah.\n",
            "  (PERSON6) Okay.\n",
            "  (PERSON10) For the real time recognition.\n",
            "  (PERSON6) Mhm, okay okay.\n",
            "  (PERSON13) So this is, this is what is then useful for SLTF for the evaluation of the simultaneity \n",
            "of of the speech.\n",
            "  (PERSON6) Okay, okay.\n",
            "  (PERSON10) So so I will.\n",
            "  I have like copy of the original TCP server that I've modified so I can do like launch [PROJECT2] script <unintelligible/>\n",
            "[deleted 18 lines]\n",
            "So [PERSON3] what what is your progress?\n",
            "  We don't hear you.\n",
            "  (PERSON13) Yeah, yeah it works.\n",
            "  Okay.\n",
            "  Hi, so I have collected, uh, Georgian and what is the name <unintelligible/> gave me.\n",
            "  Because in the last meeting we discussed like remaining languages like Georgian and Hebrew and uh-.\n",
            "  And another language, uh-.\n",
            "  (PERSON13) Icelandic.\n",
            "  (PERSON3) Icelandic, yes so it is collected and I will upload today.\n",
            "  And all the files I've already sent Russian, Serbian and what is the name.\n",
            "  Croatian.\n",
            "  [PERSON14] [PERSON14] what was the file name yeah Danish.\n",
            "  Yeah today I sent Danish file to [PERSON14] for validation.\n",
            "  As we discussed-.\n",
            "  As in the last meeting we discussed, okay, if she can do like any like similar closer languages validation.\n",
            "  (PERSON13) Yeah okay.\n",
            "  (PERSON3) So I don't know how many is-.\n",
            "  So just I received, 15 or 20 minute before I received [PERSON7] email for remaining annotators so I will send all the annotators all the files.\n",
            "  Uh, but regarding the Norwegian we didn't receive any response from the e [PERSON4] [PERSON4] sorry I don't know what is the actually [PERSON4] by [PERSON4].\n",
            "  The response.\n",
            "  And I sent it in 30th April 3000 sentences but we didn't receive any response.\n",
            "  I sent 2 reminders but I don't know.\n",
            "  (PERSON3) Busy in another work.\n",
            "  (PERSON13) Yeah, yeah.\n",
            "  Okay.\n",
            "  That's that's bad.\n",
            "  Well this can obviously happen.\n",
            "  So we have to like come-up with backup solutions and and ask people if they, uh-.\n",
            "  It's it's possible.\n",
            "  (PERSON3) She has, maybe it's possible.\n",
            "  (PERSON13) I don't know-.\n",
            "  (PERSON3) So I've asked [PERSON16] if we have any other annotator.\n",
            "  So because I've already shared some, uh, annotator list which was created by [PERSON6] I think.\n",
            "  [ORGANIZATION5] language map.\n",
            "  So once the validation is done, so we can merge like tester-.\n",
            "  (PERSON13) Yeah.\n",
            "  Again it will create problem like we count sentence alignment and so it's different status so-.\n",
            "  (PERSON13) Yeah, yeah.\n",
            "\n",
            "\n",
            "Segment 2 (Word Count: 1929):\n",
            "  Thanks for, uhm.\n",
            "  (PERSON19) Hi.\n",
            "  (PERSON19) Hi [PERSON13], I can hear you.\n",
            "  So what I have, uh, on my mind now is uh, uh, well, uh, preparations.\n",
            "  Uh, that was the, uh, the wra last part that I did.\n",
            "  Ehm, and, ehm what else, uh, the deliverables.\n",
            "  Let's let's go quickly over what what have done.\n",
            "  And then we conducted a few testing sessions with [PERSON11] and [PERSON18].\n",
            "  And they were not quite successful because the segmenters from, uh, uh, [ORGANIZATION1] they were still down and [PERSON12] today he is he is working on them.\n",
            "  (PERSON13) So, so I will make it to do-.\n",
            "  (PERSON6) So I have updated the German transcripts in its corresponding path and like do we have the golden transcripts for the English videos?\n",
            "  (PERSON13) Yes, that's the other part.\n",
            "  Because this is the consecutively translated videos.\n",
            "  Uh, so it's like like interleave the the the other way round.\n",
            "  (PERSON10) Yeah, yeah.\n",
            "  Uh, yeah, there is some some serious mismatch there.\n",
            "  So for the English ones [PERSON6], uh, I would like you to evaluate not only the word error rate of the ASR.\n",
            "  (PERSON13) Audio input.\n",
            "  From them.\n",
            "  Like, when they would be available.\n",
            "  Uh, when do [ORGANIZATION1] plus [ORGANIZATION6] expect, uh, uhm, uh, reliable run.\n",
            "  You had provided some input in the delive [PROJECT3] deliverable, right?\n",
            "  Have you done anything about that?\n",
            "  So that we have a full back-up of all the systems?\n",
            "  (PERSON6) Me?\n",
            "  (PERSON6) Yes, I I I have already did everything.\n",
            "  (PERSON13) Yeah, okay.\n",
            "  (PERSON6) That.\n",
            "  I have also did today some systems.\n",
            "  And I also did last week.\n",
            "  Yeah.\n",
            "  (PERSON13) Yeah, okay.\n",
            "  That's uh that's good.\n",
            "  Uh, so another thing is finding another partner, uh, who would ehm ehm run the replicates the replicas of our systems.\n",
            "  So we need to have someone to to replace us as, uh,-.\n",
            "  Like we are now able to run systems from [ORGANIZATION1], someone-.\n",
            "  So, uh, [PERSON13] suggest [ORGANIZATION2] to run our [PROJECT1] models but, uh, uh, but please check with [PERSON8] later on and, ehm-.\n",
            "  So [PERSON13] suggests [ORGANIZATION2] to run uh our uh our [PROJECT1] models, [ORGANIZATION1] to run our [PROJECT2] models, probably.\n",
            "  And uh uh who well uh ehm and someone to run our segmenters, yeah.\n",
            "  So, so we need to pick someone.\n",
            "  So, uh, so uh to od check with others, uh, uh tell me before uh tomm Thursday call so I know if we have a substitute for each of our systems.\n",
            "  (PERSON6) Okay.\n",
            "  (PERSON13) Yeah.\n",
            "  Okay, thank you.\n",
            "  Yeah.\n",
            "  (PERSON6) What I believe with the Czech transcripts was that is that [PERSON7] probably might have, uhm.\n",
            "  We need to ask him the audio which he used to transcribe the the videos.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON13) Yeah.\n",
            "  (PERSON6) And, and maybe that's not the real audio which he has repla he has placed in the directory.\n",
            "  (PERSON13) Yes, so that the the.\n",
            "  This is for someone who can understand Czech obviously to, uh, to find out.\n",
            "  Yeah.\n",
            "  You have been working hard <laugh/> like very hard on the evaluation of the [PROJECT1] and ASR submissions for the IWSLT last week.\n",
            "  If you can still hear us.\n",
            "  Uh.\n",
            "  (PERSON13) I can hear you.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON13) Mhm.\n",
            "  We have two ways.\n",
            "  We can <unintelligible/> ready to use tokeniser us as <unintelligible/> and so on.\n",
            "  (PERSON13) so we have discussed this.\n",
            "  So it is-.\n",
            "  So we don't trust NLTK bleu scores at all.\n",
            "  Or you have sent me an email on this?\n",
            "  I will double check.\n",
            "  SLT system mapping.\n",
            "  So I don't see any email there-.\n",
            "  <unintelligible/>\n",
            "No.\n",
            "  (PERSON19) If you want, I can resend it again.\n",
            "  (PERSON13) Space tokeniser.\n",
            "  So there is no-.\n",
            "  That is not reliable.\n",
            "  (PERSON19) Yes, but we can combine our tokeniser with NLTK.\n",
            "  (PERSON13) Uf.\n",
            "  Let's not do that.\n",
            "  (PERSON13) Mhm.\n",
            "  Yeah.\n",
            "  (PERSON13) Yes, that's it.\n",
            "  (PERSON18) Because it's much better.\n",
            "  And you can you can check my script which does tokeniser, enverse segmenter and then de-tokeniser.\n",
            "  And here is the path in the document.\n",
            "  And-.\n",
            "  (PERSON13) Yeah.\n",
            "  And it needs the the language tag as the first argument and then reference.\n",
            "  (PERSON13) Yeah.\n",
            "  So [PERSON19], do you do you fo-?\n",
            "  And this is, uh, what [PERSON18]'s script does.\n",
            "  So I would like you incorporate this logic into SLTF.\n",
            "  So that the SLTF will run the tokenisation, then enverse segmenter, then again re reconstruct the original tokenisation.\n",
            "  Which includes its own tokenisation.\n",
            "  (PERSON19) Yes, I understand.\n",
            "  (PERSON18) By by the way.\n",
            "  And we found out with [PERSON17], that if we run it without tokenisation then it works.\n",
            "  But with tokenisation it fo it like folds.\n",
            "  (PERSON18) Yeah, so it is better reliable with tokenisation but it's not 100 percent.\n",
            "  (PERSON13) Mhm.\n",
            "  (PERSON18) Robust.\n",
            "  And we can't do anything with that.\n",
            "  Except of, uh, re-implementing enverse segmenter.\n",
            "  (PERSON13) Yeah..\n",
            "(PERSON18) From scratch.\n",
            "  (PERSON13) Which.\n",
            "  Which we don't have the source code of.\n",
            "  It's a-.\n",
            "  I think it's a-.\n",
            "  Yes, but there is a paper at least.\n",
            "  So.\n",
            "  (PERSON13) Yeah, yeah okay.\n",
            "  Incorporate logic into SLTF, uh, to run enverse segmenter, uh, better, uh, include, uh, very paranoic tests, uh, of or paranoid paranoid parano hm I don't know the right spelling.\n",
            "  So that's-.\n",
            "  Because we use, uh, the, uh, can't use uh this this <unintelligible/> enverser, we need it run all section of SLT view.\n",
            "  (PERSON13) I don't understand.\n",
            "  Partly, partly because the network is is bad.\n",
            "  Uh, so maybe if you can write it down to the [ORGANIZATION4] document.\n",
            "  And I'll, I'll get back to it.\n",
            "  <unintelligible/>\n",
            "If I understood correctly, you're you are saying that, uh, there there is some like organisation issue with amber segmenter.\n",
            "  The sequence of of operations.\n",
            "  Cannot see tokenising in all-.\n",
            "  Oh, you don't have to use.\n",
            "  No no no uh.\n",
            "  No.\n",
            "  This is, this is only your, uh, your thinking.\n",
            "  But you don't have to.\n",
            "  There is-.\n",
            "  It's it's just like your design decision of SLTF.\n",
            "  It it affects many many things.\n",
            "  <laugh/>\n",
            "Who is using my log-in?\n",
            "  <laugh/>\n",
            "(PERSON6) I see in the [PROJECT3] laptop it's-.\n",
            "  Yeah.\n",
            "  <laugh/>\n",
            "(PERSON13) Yeah.\n",
            "  And write it down so that you know for the record.\n",
            "  Reading or shortening.\n",
            "  Yeah, yeah.\n",
            "  And this is like-.\n",
            "  He is only reading papers now.\n",
            "  But he'll he'll start experimenting soon.\n",
            "  Okay, that's good.\n",
            "  Uh.\n",
            "  So [PERSON19], I don't see you writing.\n",
            "  (PERSON10) Yeah, yeah, I I wrote into the ASR deliverables.\n",
            "  So that we meet the internal deadline on the 8th.\n",
            "  Yeah, okay.\n",
            "  Great.\n",
            "  Uh.\n",
            "  So [PERSON18], what what is your progress?\n",
            "  (PERSON18) Hmhm.\n",
            "  Yes, and by reading the papers I found an interesting tool.\n",
            "  (PERSON13) Mhm.\n",
            "  That's that's useful thing.\n",
            "  Uh, and later on we could even create models like-.\n",
            "  (PERSON13) So we could be reducing reducing their speech mole with a different model.\n",
            "  (PERSON18) But I have no idea how the tool works in practice.\n",
            "  (PERSON13) Yeah, uh.\n",
            "  (PERSON6) And and I worked on this.\n",
            "  But obviously more important is to have the clean um the clean cruise control, or, the set-up the set-up script over the summer.\n",
            "  (PERSON18) So so it uses the right context for previous sentences.\n",
            "  And I did I did back end text.\n",
            "  New entry point from the back end ASR which does basically the same but for text.\n",
            "  And now it now the segmenter can receive 1 message and emit 2 messages.\n",
            "  And and I connected it with on-line text flow and it works.\n",
            "  He explained me some things so I can easier fix some bugs, which we found.\n",
            "  (PERSON13) Mhm.\n",
            "  Okay.\n",
            "  (PERSON18) And we have to do the video for IWSLT by June the 17th.\n",
            "  I mean.\n",
            "  I think that's a-.\n",
            "  (PERSON6) I'm talking about the system.\n",
            "  (PERSON13) Yeah.\n",
            "  I don't know for how long the presentation should be.\n",
            "  So [PERSON18], please like propose timing for for this.\n",
            "  I don't know whether we do like regular, uh, uh, se regular slides with a with voice commentary?\n",
            "  Uh, but, uh we could also indeed concatenate it with the demo session, uh, of of the whole system.\n",
            "  And I think it would be pretty good to illustrate it how it runs live.\n",
            "  (PERSON18) It should be 15 minutes presentation.\n",
            "  That's a different.\n",
            "  Uh, um, actually so hm hmhhm.\n",
            "  I think.\n",
            "  Yeah, we should we should actually demo the, uh, the task that was in IWSLT.So some of the Antrecorp horrible performing video, uh, like horrible performing speech.\n",
            "  And also uh um the document that [PERSON6] is now measuring the word error rate and SLTF on.\n",
            "  Could could be then recorded as well and used as a part of this IWSLT presentation.\n",
            "  So I would go for like 2 minutes, uh, or 3 minutes from the 15 minute, uh, uh, talk.\n",
            "  And 1 minute of, uh, one of the Antrecorp videos.\n",
            "  Uh, that works reasonably well.\n",
            "  So this is.\n",
            "  And [PERSON6] for you it's it's nothing different from preparing the videos for the demo.\n",
            "  The only different thing is that you would also run it on the on the Antrecorp, uh, uh, speech.\n",
            "  (PERSON6) Yeah, yeah.\n",
            "  (PERSON13) So maybe [PERSON18], you have the scores for all the files, right?\n",
            "  So pick some file of Antrecorp, uh, which, uh, like, uh, works probably well.\n",
            "  The the best of Antrecorp.\n",
            "  But it's pretty bad.\n",
            "  I have only your findings paper.\n",
            "  (PERSON13) Okay.\n",
            "  Yeah, I see.\n",
            "  Uhmm.\n",
            "  (PERSON18) And validation error, only 2 documents.\n",
            "  (PERSON13) Umm, that's right.\n",
            "  (PERSON13) So this is, this is a sub-set of the lines in the output TSV file.\n",
            "  (PERSON19) Uh, yeah.\n",
            "  And then we'll choose based on the word error rate and we'll kind of assume that the translation quality is, uh, is equal across the different ASR, uh, qualities.\n",
            "  But the ASR quality is is the killing factor.\n",
            "  Thank you.\n",
            "  Uh, okay, so and now-.\n",
            "  Yeah.\n",
            "  Your, I think your microphone is is very sensitive.\n",
            "  For some reason, uh, maybe check the input, uh, again like in in Windows setting.\n",
            "  (PERSON19) Yes.\n",
            "  I'm here.\n",
            "  (PERSON13) Yes, so so-.\n",
            "  <unintelligible/>\n",
            "(PERSON13) Yeah, yeah.\n",
            "  So so I would like to have this set-up which does not rely on mediator.\n",
            "  And for that this TCP [PROJECT2] server is useful but for the mediator set-up we have the [PROJECT2] server by [PERSON18].\n",
            "  So, uh, maybe it would be good if you, uh, get in touch with [PERSON18], uh, and if the features that you have now found in [PROJECT2] server, the TCP [PROJECT2] server were were also made available if it's not too hard in the like [PROJECT2] worker.\n",
            "  Would that make sence [PERSON18]?\n",
            "  I don't know how, uh, or well please talk directly.\n",
            "  [PERSON10] and [PERSON18] should talk about this and figure out how much work would it, uh, would it involve.\n",
            "  (PERSON10) <unintelligible/> are changes that I've do I've put directly into the like binary of the TCP server.\n",
            "  But-.\n",
            "  (PERSON13) Yeah.\n",
            "  I'd like to have that, uh, uh, as well.\n",
            "  Make sure that our log files include this word level time stamps.\n",
            "  Okay, so thank you.\n",
            "  (PERSON13) Yeah.\n",
            "  Are you here?\n",
            "  No, I'm not sure.\n",
            "  (PERSON3) Hello?\n",
            "  <unintelligible/> this data.\n",
            "  So I had a conversation with [PERSON14] and she agreed for Danish and Dutch.\n",
            "  (PERSON13) Okay.\n",
            "  (PERSON3) So I just sent Danish and remaining Russian text data I've sent it has sent it to [PERSON1] for the validation.\n",
            "  Maybe she is-.\n",
            "  (PERSON13) Okay.\n",
            "  Yes, it's it's possible that like-.\n",
            "  Because we were, uh, not providing the input quickly enough, uh, that she got busy with other things.\n",
            "  We can create new tester directory and where we will put all the multilingual or parallel data equivalent.\n",
            "  Otherwise it will create-.\n",
            "\n",
            "\n",
            "Segment 3 (Word Count: 349):\n",
            "  Hello [PERSON6].\n",
            "  Thanks for joining and, uh, yeah okay.\n",
            "  Uh, and also you were evaluating-.\n",
            "  Uh, so, uh, what I think we should focus on is the demo for Project Officer.\n",
            "  Or those who of you, who are already working on the deliverables, please mention that.\n",
            "  And what what is the plan for the next week.\n",
            "  So the Czech video has been cut using the English time stamps.\n",
            "  But still, I was not expecting it to be that bad.\n",
            "  There must be some miss-match because-.\n",
            "  (PERSON6) So, like yesterday there was a holiday in Germany.\n",
            "  (PERSON19) Okay.\n",
            "  (PERSON13) So the problem is that enverse segmenter is not reliable if you run it on non-tokenised text.\n",
            "  Yeah, yeah, yeah.\n",
            "  Uh, test of uh, amber segmenter output.\n",
            "  So, uh, um, I think it's better to operate on the level of sentences and tokenise them for a purpose.\n",
            "  But then, you should again untokenise.\n",
            "  <unintelligible/>\n",
            "<laugh/>\n",
            "(PERSON13) Yeah, yeah, yeah.\n",
            "  (PERSON13) Yeah.\n",
            "  Actually [PERSON19] please write down the new assignment.\n",
            "  (PERSON19) Yeah.\n",
            "  [PERSON10] you have also written the the deliverable parts, right?\n",
            "  And there is one tool.\n",
            "  (PERSON13) Okay.\n",
            "  And-.\n",
            "  (PERSON6) Yeah, yeah, definitely, yeah.\n",
            "  So [PERSON6] can you mute?\n",
            "  Or if anyone is willing to, uh, come in.\n",
            "  If if within these 15 minutes, we have 2 minute of the 3 minute video from, uh, the Czech into English and other languages I think that would be very good.\n",
            "  So [PERSON10], you have been obviously working on your thesis.\n",
            "  Like for the emission translation or the segmentation, but for some of my experiments I've done that now if like the output gets updated much faster.\n",
            "  But I I think it uses the [PROJECT2] binary, the TCP server, right?\n",
            "  So it should be easy.\n",
            "  (PERSON10) Mhm.\n",
            "  So when we know which word as when, recognised when, we can link it and, uh, score it against the golden transcript.\n",
            "  (PERSON3) Okay.\n",
            "  And okay, that is working-.\n",
            "  I remain.\n",
            "  I have-.\n",
            "  Uh, uh, so I have already shared if you agree on that unle annotator so we can send all the files for validations.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Compute the centroid for each segment (average of sentence embeddings)\n",
        "segment_centroids = []\n",
        "for i, segment in enumerate(segmented_text):\n",
        "    # Get the embeddings of sentences in this segment\n",
        "    segment_embeddings = [sentence_embeddings[j] for j, cluster_id in enumerate(cluster_assignment) if cluster_id == i]\n",
        "\n",
        "    # Compute the centroid (mean of all embeddings in the segment)\n",
        "    centroid = np.mean(segment_embeddings, axis=0)\n",
        "    segment_centroids.append(centroid)\n",
        "\n",
        "# Step 2: Score sentences based on cosine similarity with the centroid of their respective segments\n",
        "sentence_scores = []\n",
        "for i, segment in enumerate(segmented_text):\n",
        "    segment_embeddings = [sentence_embeddings[j] for j, cluster_id in enumerate(cluster_assignment) if cluster_id == i]\n",
        "\n",
        "    # Compute cosine similarity between each sentence embedding and the segment's centroid\n",
        "    similarities = cosine_similarity(segment_embeddings, [segment_centroids[i]]).flatten()\n",
        "\n",
        "    # Pair each sentence with its similarity score\n",
        "    sentence_score_pairs = list(zip(segment, similarities))\n",
        "\n",
        "    # Sort sentences by similarity score (descending)\n",
        "    sentence_score_pairs = sorted(sentence_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Store the scores for this segment\n",
        "    sentence_scores.append(sentence_score_pairs)\n",
        "\n",
        "# Step 3: Select top sentences for summarization\n",
        "summary = []\n",
        "for i, segment_scores in enumerate(sentence_scores):\n",
        "    print(f\"\\nTop sentences for Segment {i + 1}:\")\n",
        "\n",
        "    # Adjust 'n_sentences' to control how many top sentences you want per segment\n",
        "    n_sentences = 2  # For example, select the top 2 sentences from each segment\n",
        "    top_sentences = segment_scores[:n_sentences]\n",
        "\n",
        "    for sentence, score in top_sentences:\n",
        "        summary.append(sentence)  # Add top sentence to summary\n",
        "        print(f\"  {sentence} (Score: {score})\")\n",
        "\n",
        "# Join the top sentences from all segments to form the final summary\n",
        "final_summary = ' '.join(summary)\n",
        "print(\"\\nFinal Summary:\")\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2vgv5-1Sp7E",
        "outputId": "5ccadaa3-b195-4ba3-8485-32a17f1cc09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top sentences for Segment 1:\n",
            "  (PERSON13) Yeah. (Score: 0.9487306475639343)\n",
            "  (PERSON13) Yeah. (Score: 0.9487306475639343)\n",
            "\n",
            "Top sentences for Segment 2:\n",
            "  And uh uh who well uh ehm and someone to run our segmenters, yeah. (Score: 0.6205377578735352)\n",
            "  (PERSON13) Yeah, so this is, this is an important, uh, task, uh, to do, uh wr also for German and English audios. (Score: 0.6151423454284668)\n",
            "\n",
            "Top sentences for Segment 3:\n",
            "  Yeah. (Score: 0.9586171507835388)\n",
            "  Yeah. (Score: 0.9586171507835388)\n",
            "\n",
            "Final Summary:\n",
            "(PERSON13) Yeah. (PERSON13) Yeah. And uh uh who well uh ehm and someone to run our segmenters, yeah. (PERSON13) Yeah, so this is, this is an important, uh, task, uh, to do, uh wr also for German and English audios. Yeah. Yeah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def rank_sentences_tfidf(sentences):\n",
        "    # Apply TF-IDF to sentences\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Calculate importance scores by summing the TF-IDF values for each sentence\n",
        "    sentence_scores = tfidf_matrix.sum(axis=1).flatten().tolist()\n",
        "\n",
        "    # Rank sentences by score in descending order\n",
        "    ranked_sentences = sorted(zip(sentence_scores, sentences), reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    return ranked_sentences\n",
        "\n",
        "# Example usage: rank sentences in each segment\n",
        "for i, segment in enumerate(segmented_text):\n",
        "    ranked_sentences = rank_sentences_tfidf(segment)\n",
        "\n",
        "    # Extract top N sentences to form the summary (e.g., top 3 sentences)\n",
        "    top_sentences = [sentence for score, sentence in ranked_sentences[:3]]\n",
        "    summary = \" \".join(top_sentences)\n",
        "\n",
        "    print(f\"Summary for Segment {i+1}:\")\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aya2WHjRUJjV",
        "outputId": "fc40c582-b548-4c2d-cd81-4fc0381deb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for Segment 1:\n",
            "(PERSON13) Hi.\n",
            "\n",
            "\n",
            "Summary for Segment 2:\n",
            "Thanks for, uhm.\n",
            "\n",
            "\n",
            "Summary for Segment 3:\n",
            "(PERSON13) Yeah, great.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def extractive_summary(segmented_text, num_sentences=2):\n",
        "    summaries = []\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "    for segment in segmented_text:\n",
        "        if segment:\n",
        "            # Step 1: Generate embeddings for all sentences in the segment\n",
        "            sentence_embeddings = model.encode(segment)\n",
        "\n",
        "            # Step 2: Calculate the centroid of the embeddings\n",
        "            centroid = np.mean(sentence_embeddings, axis=0)\n",
        "\n",
        "            # Step 3: Calculate cosine similarities of each sentence to the centroid\n",
        "            similarities = cosine_similarity([centroid], sentence_embeddings)[0]\n",
        "\n",
        "            # Step 4: Get indices of the top 'num_sentences' most similar sentences\n",
        "            top_indices = similarities.argsort()[-num_sentences:][::-1]\n",
        "\n",
        "            # Step 5: Create the summary by selecting the top sentences\n",
        "            segment_summary = ' '.join([segment[i] for i in sorted(top_indices)])\n",
        "            summaries.append(segment_summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage\n",
        "summarized_minutes = extractive_summary(segmented_text, num_sentences=2)\n",
        "\n",
        "# Print the summaries\n",
        "print(\"\\nImproved Extractive Summarized Minutes:\")\n",
        "for i, summary in enumerate(summarized_minutes):\n",
        "    print(f\"Segment {i + 1} Summary: {summary}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st-Ku6sDUKIc",
        "outputId": "eb5893de-c945-4bba-805c-ef4d8fadb6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Improved Extractive Summarized Minutes:\n",
            "Segment 1 Summary: (PERSON13) Yeah, yeah. (PERSON13) Yeah, yeah.\n",
            "\n",
            "Segment 2 Summary: (PERSON13) Yeah. (PERSON13) Yeah, uh.\n",
            "\n",
            "Segment 3 Summary: (PERSON13) Yeah. (PERSON19) Yeah.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(segment):\n",
        "    \"\"\"Remove stopwords from the segment.\"\"\"\n",
        "    filtered_segment = []\n",
        "    stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "\n",
        "    for sentence in segment:\n",
        "        # Tokenize sentence into words\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        # Remove stopwords\n",
        "        filtered_sentence = ' '.join([word for word in words if word not in stop_words])\n",
        "        if filtered_sentence:  # Only keep non-empty sentences\n",
        "            filtered_segment.append(filtered_sentence)\n",
        "    return filtered_segment\n",
        "\n",
        "def extractive_summary(segmented_text, num_sentences=2):\n",
        "    summaries = []\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "    for segment in segmented_text:\n",
        "        # Remove stopwords from each segment\n",
        "        filtered_segment = remove_stopwords(segment)\n",
        "\n",
        "        if filtered_segment:\n",
        "            # Step 1: Generate embeddings for all filtered sentences in the segment\n",
        "            sentence_embeddings = model.encode(filtered_segment)\n",
        "\n",
        "            # Step 2: Calculate the centroid of the embeddings\n",
        "            centroid = np.mean(sentence_embeddings, axis=0)\n",
        "\n",
        "            # Step 3: Calculate cosine similarities of each sentence to the centroid\n",
        "            similarities = cosine_similarity([centroid], sentence_embeddings)[0]\n",
        "\n",
        "            # Step 4: Get indices of the top 'num_sentences' most similar sentences\n",
        "            top_indices = similarities.argsort()[-num_sentences:][::-1]\n",
        "\n",
        "            # Step 5: Create the summary by selecting the top sentences\n",
        "            segment_summary = ' '.join([filtered_segment[i] for i in sorted(top_indices)])\n",
        "            summaries.append(segment_summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage\n",
        "summarized_minutes = extractive_summary(segmented_text, num_sentences=2)\n",
        "\n",
        "# Print the summaries\n",
        "print(\"\\nImproved Extractive Summarized Minutes:\")\n",
        "for i, summary in enumerate(summarized_minutes):\n",
        "    print(f\"Segment {i + 1} Summary: {summary}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fadgkhp31FMS",
        "outputId": "a6e9782e-a6da-4d24-fc9c-b3f477513d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Improved Extractive Summarized Minutes:\n",
            "Segment 1 Summary: ( person13 ) yeah , yeah . ( person13 ) yeah , yeah .\n",
            "\n",
            "Segment 2 Summary: ( person13 ) yeah , uh . ( person13 ) yeah .\n",
            "\n",
            "Segment 3 Summary: ( person13 ) yeah . ( person19 ) yeah .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the model**"
      ],
      "metadata": {
        "id": "AJeduu_T3AdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dataset_path.xlsx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QsS2kJhaRjN",
        "outputId": "efc12781-a185-4306-9a8e-f920f2cd3a1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dataset_path.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6dcsKSr51un",
        "outputId": "9ae95eaa-9045-4354-d0dc-a5aca53797c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHBSwFPvS0q9",
        "outputId": "7dba3aa1-4902-4688-d1a9-d3d7c165ec13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Thb0htp55L1Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base paths for the datasets (adjust if needed)\n",
        "train_base_path = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/train/'\n",
        "dev_base_path = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dev/'\n",
        "test_base_path = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/test/'\n",
        "\n",
        "# Load the Excel file and read the \"train\" and \"dev\" sheets\n",
        "df_train = pd.read_excel('/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dataset_path.xlsx', sheet_name='train')\n",
        "df_dev = pd.read_excel('/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dataset_path.xlsx', sheet_name='dev')\n",
        "df_test = pd.read_excel('/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dataset_path.xlsx', sheet_name='test_1')"
      ],
      "metadata": {
        "id": "rpPpOvmPnaKp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset by reading the transcript and summary files\n",
        "def prepare_dataset(df, base_path):\n",
        "   dataset = []\n",
        "\n",
        "   for index, row in df.iterrows():\n",
        "       folder_name = row['folder_name']\n",
        "       transcript_file = row['transcripts']\n",
        "       summary_file = row.get('summary_1', None)  # Only for train and dev\n",
        "\n",
        "\n",
        "       transcript_path = os.path.join(base_path, folder_name, transcript_file)\n",
        "\n",
        "\n",
        "       with open(transcript_path, 'r', encoding='utf-8') as transcript_f:\n",
        "           transcript_text = transcript_f.read()\n",
        "\n",
        "       if summary_file:  # If summary file exists (train/dev)\n",
        "           summary_path = os.path.join(base_path, folder_name, summary_file)\n",
        "           with open(summary_path, 'r', encoding='utf-8') as summary_f:\n",
        "               summary_text = summary_f.read()\n",
        "           dataset.append({'transcript': transcript_text, 'summary': summary_text})\n",
        "       else:  # For test dataset (no summary)\n",
        "           dataset.append({'transcript': transcript_text})\n",
        "\n",
        "   return Dataset.from_list(dataset)"
      ],
      "metadata": {
        "id": "QGjO3h6qntbU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare train, dev, and test datasets\n",
        "train_dataset = prepare_dataset(df_train, train_base_path)\n",
        "dev_dataset = prepare_dataset(df_dev, dev_base_path)\n",
        "test_dataset = prepare_dataset(df_test, test_base_path)\n",
        "\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "model_name = \"facebook/bart-large\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGuH8HJi7U2q",
        "outputId": "36153d0e-65f2-452c-9ba5-ab4590d5ab91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function for datasets\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the input (transcripts)\n",
        "    inputs = tokenizer(examples['transcript'], padding=\"max_length\", truncation=True, max_length=1024)\n",
        "\n",
        "    # Tokenize the labels (summaries) if available\n",
        "    if 'summary' in examples:\n",
        "        labels = tokenizer(examples['summary'], padding=\"max_length\", truncation=True, max_length=150)\n",
        "        inputs['labels'] = labels['input_ids']\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "K1u35LBoKwRC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets\n",
        "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
        "dev_tokenized = dev_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch tensors\n",
        "train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dev_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f2dde1cb63fa4c23848150f18d774c81",
            "9c8289c6ff83476db02a809fd083e4d1",
            "0dacaa93e763448997130dafa8352bb7",
            "9faf6c50f6074807b9812c6acfc8c176",
            "e0b07f45ad4b42fb8156fefeac18e3b4",
            "aa6e5962bb2d456f8a34ee5e3f92a36c",
            "9d97231fde2446f4b57f3b0b639fe516",
            "9584871d204740358bfc60d18a943ec1",
            "d03aa8515d35448b8482ed6425cd8e08",
            "7aa425df565c47a4921652775a7459db",
            "9e8556cc0a944848ae6d9a3780af5737",
            "fe8413bac1494f35addaf6c62cc6d2f5",
            "0e72f37ac1ad4a018daed2ac32b9ca77",
            "3a53b79757da43738ada5f8d35c1a07d",
            "c1d1c03836f2401abee06528d670ece1",
            "0af62618b18a4677b0b0a48dd1e43f11",
            "00a7f2d6987648f5b3fa3ab35e43e65a",
            "bbea22768ce24173b508358a6096fb45",
            "d0f98e51c0fa4b2dbe386794c269900b",
            "e1cd04469be444039bd5f6521df0edd0",
            "308e8d6388544af486d1f6998d2ec204",
            "969cdbf6343942a6a5c2d7f2842f86b8"
          ]
        },
        "id": "HUcmvIr_095r",
        "outputId": "aca991a6-8696-470d-e9b8-bd18f6ac8406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/84 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2dde1cb63fa4c23848150f18d774c81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe8413bac1494f35addaf6c62cc6d2f5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,  # Reduce batch size\n",
        "    per_device_eval_batch_size=2,   # Reduce evaluation batch size\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    fp16=True,  # Enable mixed precision training\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUzEVxMBz9x1",
        "outputId": "23af5ac1-38f2-4720-a192-e0a11901ee75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=dev_tokenized,\n",
        ")"
      ],
      "metadata": {
        "id": "mbvi_A1-0EEj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Clear GPU cache before training (optional)\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "ljO5e2Kq0JHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize test dataset (no labels)\n",
        "test_tokenized = test_dataset.map(\n",
        "    lambda examples: tokenizer(examples['transcript'], padding=\"max_length\", truncation=True, max_length=1024),\n",
        "    batched=True\n",
        ")\n",
        "test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "UF3-HgBl1qHS",
        "outputId": "97f2a946-4cc5-4687-df61-f53e6c19f305"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9bad979f8b34>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize test dataset (no labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m test_tokenized = test_dataset.map(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries for the test set\n",
        "def generate_summary(test_data):\n",
        "    inputs = tokenizer(test_data['transcript'], return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    input_ids = inputs['input_ids'].to(model.device)\n",
        "    attention_mask = inputs['attention_mask'].to(model.device)\n",
        "\n",
        "    # Generate summary (greedy decoding)\n",
        "    summary_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_beams=4)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "XUOqV3hR12vX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics using evaluate library\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825,
          "referenced_widgets": [
            "6e79699200e74792aab0186e0a206e7d",
            "607210f5311b444c833b4efac4c57920",
            "1f19a668dafb4e7f8d1f337de2d5d920",
            "5c07f80af7ec4cab8563cb8f99028ca7",
            "af5c9e2a0a1a4f15848152d5391fad92",
            "f6ccc4eed4114f3e9acc0b07476f1a1e",
            "b3b7cd3d8e0e4af3ba869acb964170af",
            "e370b655920947a189b5656687d79279",
            "e4d023e6d09c4f4ea71345a20b8332de",
            "410ea1f6654541aea65f8c6ca07e41a9",
            "b4fb4125d3ac4f09bd78cff6d9d32f8d"
          ]
        },
        "id": "6iqiJyjfW4dB",
        "outputId": "c89f0231-57c0-43d5-e148-53f317791c64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e79699200e74792aab0186e0a206e7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "To be able to use evaluate-metric/rouge, you need to install the following dependencies['rouge_score'] using 'pip install rouge_score' for instance'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c9d0141234b2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluation metrics using evaluate library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bleu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \"\"\"\n\u001b[1;32m    747\u001b[0m     \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     evaluation_module = evaluation_module_factory(\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m             raise FileNotFoundError(\n\u001b[1;32m    682\u001b[0m                 \u001b[0;34mf\"Couldn't find a module script at {relative_to_absolute_path(combined_path)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                 \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                                 \u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m                             ).get_module()\n\u001b[0m\u001b[1;32m    640\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mimports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_imports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         local_imports = _download_additional_modules(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_hub_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mneeds_to_be_installed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary_import_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_import_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneeds_to_be_installed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;34mf\"To be able to use {name}, you need to install the following dependencies\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;34mf\"{[lib_name for lib_name, lib_path in needs_to_be_installed]} using 'pip install \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: To be able to use evaluate-metric/rouge, you need to install the following dependencies['rouge_score'] using 'pip install rouge_score' for instance'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries for each entry in the test set\n",
        "for i, test_example in enumerate(test_dataset):\n",
        "    summary = generate_summary(test_example)\n",
        "    print(f\"Test Example {i+1}:\")\n",
        "    print(summary)\n",
        "    print(\"-------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6TaXqOF15wB",
        "outputId": "f23d3578-b7f9-4b43-e70c-15739bcdc753"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Example 1:\n",
            "MEETING MINUTES\n",
            "Attendees:\n",
            "[PERSON5], [PERSON23], [ORGANIZATION1], [PROJECT2]\n",
            "Purpose of meeting: To go through the topics in the document, to see what is working and what is not working.\n",
            "\n",
            "Meeting Participants:\n",
            "\n",
            "\n",
            "-   \n",
            "\n",
            "-- \n",
            "-- [Mentioned in the meeting:\n",
            "-- - \n",
            "--- \n",
            "\t--  \t\t\t\n",
            "\t- \n",
            "-------------------------------------------------------\n",
            "Test Example 2:\n",
            "[PROJECT3], [PROJECT7], [ORGANIZATION4], [PERSON11], [PRODUCT1]\n",
            "Purpose of the meeting: To discuss the progress of the [PROPROJECT1] project\n",
            "Date: 09. 05. 2020\n",
            "Attendees:\n",
            "[PERSON6], [MALE1], [MEETING2], [REPRESENTATION4]\n",
            "Meeting Topic: [PROOPER1] and [PROProject5]\n",
            "Date of meeting: 08. 04. 2020.\n",
            "Meets Participants:\n",
            "Attended: [PORGANIZEATION8], [ANNOTATOR6], (PERSON3], (MALE2), [\n",
            "-------------------------------------------------------\n",
            "Test Example 3:\n",
            "[PROJECT3] \n",
            "Attendees:\n",
            "[PERSON10], [PERSON8], [ORGANIZATION1], [PROJECT6]\n",
            "Purpose of meeting: To discuss progress of the [PROPROJECT1] worker and to discuss the proposed development schedule.\n",
            "\n",
            "\n",
            "Date: 09. 05. 2020\n",
            "Meeting Name: [PROOPERATION1]   \n",
            "Date of the meeting: 08. 05\n",
            "-------------------------------------------------------\n",
            "Test Example 4:\n",
            "MEETING MINUTES\n",
            "\n",
            "Attendees: [PERSON10], [ORGANIZATION6], [PORGANIZEATION7], [PROJECT3], [PRODUCT1]\n",
            "Purpose of meeting: To discuss the status of the presentation platform and to discuss the upcoming workshop.\n",
            "Meeting Name: Presentation Platform \n",
            "Date: 12. 12. 2020\n",
            "Meets Participants: [ORPROJECT7], (PERSON13), [PPROJECT4], [PORGANIZATIONS1], (PROJECT2], (PRODUCT3), (PRINCIPAL2), (PREPORTABLE2)\n",
            "Purposes of meeting : To discuss progress on the project.\n",
            "\n",
            "-------------------------------------------------------\n",
            "Test Example 5:\n",
            "MEETING MINUTES\n",
            "Attendees: [PERSON12], [ORGANIZATION4], [PORGANIZEMENT2], [PROJECT1]\n",
            "Purpose of meeting: Monitoring of ASR worker and machine translation into Czech\n",
            "Meeting Name: Monitoring the ASR workers\n",
            "Date: 09. 05. 2020\n",
            "Meets Participants: [ORIGATION4]\n",
            "Meaning of meeting, purpose of meeting and what to be discussed: Monitoring problem.\n",
            "-- Monitoring problem has been fixed.\n",
            "\n",
            "-- The ASR processing is done two times now, and it will be done three times with the German one.\n",
            "\t-- The machine translation worker is also doing the Czech translation.\n",
            "\n",
            "-------------------------------------------------------\n",
            "Test Example 6:\n",
            "MEETING MINUTES\n",
            "Attendees: [PERSON11], [PORGANIZATION3], [ORGANIZEATION8], [PROJECT2]\n",
            "Purpose of meeting: To discuss the status of the project and to discuss the next steps.\n",
            "Meeting Topic:   \n",
            "   [ORIGATION3]  \n",
            "-------------------------------------------------------\n",
            "Test Example 7:\n",
            "MEETING MINUTES\n",
            "Meeting Name: [PROJECT1] Meeting Date: 12. 12. 2020\n",
            "Meets Participants: [PERSON10], [PORGANIZATION2], [ORGANISATION3], [PORGANIZATIONS1], [PROJECTS2], (ORGANIZEATION3]\n",
            "Meaning of meeting:\n",
            "• There will be call on Thursday.\n",
            "• It should be around 1 P.M. of our [LOCATION2] time, I'm not sure, what it is called.\n",
            "\t• There should be at least 10 people present.\n",
            "\n",
            "\t\t• It is possible that you're not in the mailing list or, yet.\n",
            "-------------------------------------------------------\n",
            "Test Example 8:\n",
            "MEETING MINUTES\n",
            "Attendees:\n",
            "[PERSON5], [PERSON8], [ORGANIZATION3], [PROJECT3]\n",
            "Meeting Date: 11. 10. 2020\n",
            "Purpose of meeting: To discuss the manual evaluation thing and how to implement the negotation.\n",
            "\n",
            "Date: 09. October. 2020.\n",
            "Presentations:\n",
            "\n",
            "\n",
            "-   \n",
            "\n",
            "• \n",
            "-\n",
            "- Overview of the evaluation thing.\n",
            "-- \n",
            "\t- \n",
            " - \n",
            " \n",
            "--\n",
            "- [PREPORTER1] is searching for some reference to the evaluation.\n",
            "\t--  \t\n",
            "\n",
            "\t\t\n",
            "-- [P\n",
            "-------------------------------------------------------\n",
            "Test Example 9:\n",
            "MEETING MINUTES\n",
            "Meeting Name: [PROJECT1] 2019\n",
            "Attendees: [PERSON10], [PORGANIZATION3], [ORGANIZEATION3]\n",
            "Purpose of meeting: To discuss problems with the presentation platform, to discuss solution to the problem with the [PROPOSED] presentation platform.\n",
            "\n",
            "Problem with the platform\n",
            "Problem of the platform:   \n",
            "[PERSON13] is not present at the meeting yet.\n",
            "• The platform is not ready for the workshop yet. \n",
            "• There are several problems with workers.\n",
            "\t• The problem with ASR workers is the missing multiplexing\n",
            "\t\t\t\n",
            "\t\n",
            "\n",
            "\t\n",
            "-------------------------------------------------------\n",
            "Test Example 10:\n",
            "MEETING MINUTES\n",
            "\n",
            "Attendees: [PERSON3], [ORGANIZATION6], [PANORATION2], [ANNOTATOR1], [PROJECT2]\n",
            "Purpose of meeting: To discuss the status of the project and to discuss the budget. \n",
            "Meeting Participants:\n",
            "[PERSON1]\n",
            "-   \n",
            "-- \n",
            "\n",
            "-- [PENORATOR1]\t--  \t- \n",
            "- [PONORATOR2]\t\n",
            "-\n",
            "-------------------------------------------------------\n",
            "Test Example 11:\n",
            "[PROJECT5], [ORGANIZATION1], [PROJECT2]\n",
            "Attendees:\n",
            "[PERSON7], [PERSON9], (PERSON6], (ANNOTATOR4], (ORGANIZEATION5], (PROJECT3), (PANSHARING1), (ORIGINATION2), (ANTRAVISATION1)\n",
            "Purpose of meeting: To discuss about the topics of the project.\n",
            "\n",
            "\n",
            "- [PREPORIZATION2] is working on transformer networks.\n",
            "-- [PPROJECT4] is also working on transformers.\n",
            "\t- [ORPROJECT1] is doing some probing on transformer network.\n",
            "\n",
            "-------------------------------------------------------\n",
            "Test Example 12:\n",
            "MEETING MINUTES\n",
            "Attendees:\n",
            "[PERSON12], [PERSON20], [ORGANIZATION8], [PROJECT1], [ROJECT1]\n",
            "Purpose of meeting: To discuss the situation in Czech Republic.\n",
            "Meeting Participants:\n",
            "\n",
            "\n",
            "-   \n",
            "\n",
            "-- \n",
            "--- \n",
            "--\n",
            "---\n",
            "---\n",
            "-------------------------------------------------------\n",
            "Test Example 13:\n",
            "[PROJECT2] [ORGANIZATION5] workshop\n",
            "Attendees:\n",
            "[PERSON8], [PERSON14], [PROJECT3], [ORPROJECT4], [PORGANIZATIONS2], [PRODUCT1], (PERSON6], (PRODUCT2], (PROJECT1)\n",
            "Purpose of meeting:   \n",
            "\n",
            "\n",
            "- Discussion about the work.\n",
            "-   [PREPORATION1] -   Overview of the project.\n",
            "-- \n",
            "- [PPROJECT6] - [PORGANIZEATION2] is working on audio submition.\n",
            "\n",
            "-- [POPULAR1] is also working on\n",
            "-------------------------------------------------------\n",
            "Test Example 14:\n",
            "[PROJECT2] [PROJECT1] meeting\n",
            "Attendees: [PERSON8], [ORGANIZATION2], [PORGANISATION3], [PORGANIZATIONS4], [PRINCIPAL1], [ANNOTATOR5], [APPROJECT6], [OPEN MINUTES4],\n",
            "Purpose of meeting: To collect meetings\n",
            "Meeting purpose: to collect meetings and to discuss important documents\n",
            "Mentioned:\n",
            "[PERSON5] is collecting various meetings.\n",
            "• [PRODUCT4] is working on the [PROPERTY2] project.\n",
            "\t• [PONORATOR6] is also working on it.\n",
            "\n",
            "\n",
            "-------------------------------------------------------\n",
            "Test Example 15:\n",
            "MEETING MINUTES\n",
            "Attendees:\n",
            "[PERSON8], [PERSON9], [ORGANIZATION2], [PROJECT2]\n",
            "Purpose of meeting: Introduction of new content into the book.\n",
            "Meeting Participants:\n",
            "\n",
            "• Introduction is to do.\n",
            "• Multilinguality is not there at all.\n",
            "\t• The problem is that we haven't that defined what does it mean to add new content, just write this nicely.\n",
            "\n",
            "\t\t\t\n",
            "\tSummary of the meeting:\n",
            "• There is no deadline for, um, stop adding new content in the book, but we should, in the next week we should -\n",
            "\t\n",
            "• The multilinguality\n",
            "-------------------------------------------------------\n",
            "Test Example 16:\n",
            "MEETING MINUTES\n",
            "Meeting Name: [ORGANIZATION1] \n",
            "Date: 12. 12. 2020\n",
            "Attendees: [PERSON10], [PORGANIZEATION2], [ORIGINAL1], [PROJECT1]\n",
            "Purpose of the meeting:   \n",
            "\n",
            "\n",
            "•   [PONORIZATION2]  \t• \n",
            "\t\t\t\n",
            "-------------------------------------------------------\n",
            "Test Example 17:\n",
            "MEETING MINUTES\n",
            "Attendees:\n",
            "[PERSON6], [PERSON11], [ORGANIZATION3], [PROJECT3]\n",
            "Purpose of meeting: Preparation for the start project.\n",
            "\n",
            "Meeting Name: Start Project\n",
            "Date: 09. 09. 2020\n",
            "Meets Participants:\n",
            "• [PENORMATE1], [PORGANIZATOR2], [NORTHERNIZATION4], [PUBLICIZATION2]\n",
            "-------------------------------------------------------\n",
            "Test Example 18:\n",
            "MEETING MINUTES\n",
            "Date: 20. 05. 2020\n",
            "Attendees: [PERSON7], [PORGANIZATION12], [ORGANIZEATION4], [PROJECT4]\n",
            "Purpose of meeting: To discuss the status of the project, to check the functionalities to be checked, to discuss what what we need in the platform and to find a slot for the technical discussion.\n",
            "Meeting Participants:\n",
            "[PERSON12]\n",
            "Date of the meeting: 20th of May 2020\n",
            "Presentation platform, \n",
            "Function analysis, \n",
            "-------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROUGE score\n",
        "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu_scores = bleu.compute(predictions=predictions, references=references)\n",
        "print(\"BLEU Scores:\", bleu_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "vjIQTWqrXBNo",
        "outputId": "955cf2c1-50d4-4889-cd78-808d2328f13a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rouge' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ed56981bcd5f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute ROUGE score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrouge_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROUGE Scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute BLEU score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rouge' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory to save generated summaries\n",
        "output_summary_dir = '/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/test/generated_summaries/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(output_summary_dir, exist_ok=True)\n",
        "\n",
        "# Function to generate and save summary for a test example\n",
        "def generate_and_save_summary(test_data, index):\n",
        "    # Generate the summary\n",
        "    inputs = tokenizer(test_data['transcript'], return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    input_ids = inputs['input_ids'].to(model.device)\n",
        "    attention_mask = inputs['attention_mask'].to(model.device)\n",
        "\n",
        "    # Generate the summary (beam search)\n",
        "    summary_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_beams=4)\n",
        "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Define the filename for the summary file\n",
        "    summary_filename = f\"generated_summary_{index+1}.txt\"\n",
        "    summary_filepath = os.path.join(output_summary_dir, summary_filename)\n",
        "\n",
        "    # Save the generated summary to a file\n",
        "    with open(summary_filepath, 'w', encoding='utf-8') as summary_file:\n",
        "        summary_file.write(summary_text)\n",
        "\n",
        "    # Return the summary file path\n",
        "    return summary_filepath\n",
        "\n",
        "# List to hold the paths of generated summaries\n",
        "generated_summaries = []\n",
        "\n",
        "# Generate summaries for each transcript in the test set and save them\n",
        "for i, test_example in enumerate(test_dataset):\n",
        "    summary_filepath = generate_and_save_summary(test_example, i)\n",
        "    generated_summaries.append(summary_filepath)\n",
        "\n",
        "# Add the generated summaries file paths to the 'summary_1' column in df_test\n",
        "df_test['summary_1'] = generated_summaries\n",
        "\n",
        "# Save the updated df_test with the summary file paths\n",
        "df_test.to_excel('/content/drive/MyDrive/ELITR_dataset/elitr-minuting-corpus-en/dataset_path_updated_test.xlsx', index=False)\n",
        "\n",
        "print(\"Summaries generated and saved successfully. df_test updated with summary file paths.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFPD0CM5cIHZ",
        "outputId": "54ef4bfa-6067-4ebb-ff46-15e7cb09041d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries generated and saved successfully. df_test updated with summary file paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNt1BBjm5qHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}